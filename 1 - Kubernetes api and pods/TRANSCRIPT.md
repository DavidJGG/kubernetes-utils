### Course Overview

Hi everyone. My name is Anthony Nocentino, Enterprise Architect and Founder of Centino Systems. Welcome to my course, Managing the Kubernetes API Server and Pods. Are you a systems administrator or a developer that needs to deploy workloads in Kubernetes clusters? If you do, then this is the course for you. First, we'll start off by digging into the Kubernetes API server and learn how it works. We'll learn how to interact with it and its internals of its critical functions in our Kubernetes cluster. Then, we'll learn how to use labels, annotations, and namespaces to help get control over and organize workloads in our clusters. And to wrap it up, we'll look closely at the primary workload construct in Kubernetes, the Pod. We'll learn what they're made of, how they work, and how to keep them online in reporting our application health. By the end of this course, you'll have what it takes to debug issues with your API server and your cluster workloads. You'll learn how to organize and get your arms around your largest Kubernetes workloads and also how to craft the best Pods for your applications. Before this course, you should be familiar with the Linux operating system and administering it at the command line. You should have a firm understanding of TCP/IP‑based networking, and also the fundamental concepts of containers. You'll need to know the core foundations of Kubernetes, like what a cluster is and how to interact with it at the command line. I hope you'll join me on this journey to learn how to deploy and manage workloads in Kubernetes in the course, Managing the Kubernetes API Server and Pods.

### Introduction, Course, and Module Overview

Hello. This is Anthony Nocentino with Centino Systems. Welcome to my course, Managing the Kubernetes API Server and Pods. In this module, we're going to kick it off with a course introduction and then dive right into the content with Using the Kubernetes API. All right, so let's start the course off with a course overview, and we're going to kick things off with the module Using the Kubernetes API. We're going to look at how we can use the Kubernetes API to build and model systems to deploy into our Kubernetes cluster. And then we'll also take a close look at the API server itself and its internal functions. As our Kubernetes workloads grow, we'll need to use some techniques to help interact with and manage our deployed applications and services in our cluster, and we'll learn how to do that in the module Managing Objects with Labels, Annotations, and Namespaces. And then finally, we'll take a deep‑dive look at the primary workload element in Kubernetes, the Pod, in the module Running and Managing Pods. So let's take a look at an overview for this module. And first up, we're going to talk about the Kubernetes API and the API server where we'll take a closer look at the API itself, API objects, and the internals of the API server. Next up is we'll look at working with Kubernetes objects. We'll look at the types of objects available, how to use them, looking closely at how we define objects, Kubernetes API groups, and also how the API server itself is versioned. Then we'll wrap up the module with a deep dive into the anatomy of an API request where we'll look closely at what happens when we submit a request into the API server.

### The Kubernetes API and API Server

The Kubernetes API is a single surface area over the resources that are available in our data center or, more specifically, inside of our Kubernetes cluster. The Kubernetes API gives us the ability to use API objects to help model our system and build the workloads that we want to deploy in our cluster. The API objects are a collection of primitives to help us represent our system state where we can define what that desired state is, enabling us to configure our system and model it as API objects deployed in our API server. The API server is going to be the sole way that we interact with our cluster, and it's also the sole way that Kubernetes interacts with our cluster when it needs to exchange data between the various components of the cluster itself. So let's take a closer look at the Kubernetes API server. The Kubernetes APIs server is a client/server architecture. It implements a RESTful API over HTTP using JSON. So we're going to exchange JSON objects between client and server using our RESTful API, so GET, SET, and POST operations and things like that. We'll look much more closely at those operations later on in this module. Now the client is going to submit requests to the server over HTTP or HTTPS. The server is going to respond to these requests based on the actions it had to take, whether there was a positive result and made the change that was requested or perhaps a negative result, and there was an error or resource that wasn't found. We're going to look much more closely at responses later on in this module as well. The API server itself is stateless. So what this means is any configuration changes that we've made in our cluster via API objects aren't stored in the API server. They're actually serialized and persisted into the cluster data store, which is traditionally etcd.

### The Control Plane and API Objects

So let's take a look more practically at the control plane node in our cluster. Inside the control plane node, we have a collection of critical services that help us facilitate cluster operations, the API server, the cluster store, the scheduler, and the controller manager. The API server is the primary access point for administrative and cluster operations. This is the communication hub of the entire Kubernetes system. The cluster store is where the state of the system is stored and persisted in a key value data store, etcd. The scheduler tells Kubernetes which nodes to start Pods on based on the Pod's resource requirements in our workloads. And then finally, the controller manager has the job of implementing the lifecycle functions of controllers, basically keeping things in the desired state. In this module we're going to focus on the API server and its interactions with clients like kubectl and how they work together to create or make changes to resources deployed in our cluster with API objects. Kubernetes API objects are the persistent entities that allow us to build and deploy systems in Kubernetes. They are the things that we'll use in our code to model our systems. We'll send that code into the API server, and Kubernetes will persist that configuration in the cluster store through the API server and go about building our system in our desired state. So, the Kubernetes API objects are the things that represent the state of the system that we want to build. Kubernetes API objects are organized in three ways, and we're going to look at each one of these individually as we go through the next series of slides. And first up is kind. Kind is a string value representing the REST resource this object represents in the API server, so basically the things that we've been working with so far earlier in this series of courses, things like Pods, services, deployments, and so on, all the API objects that are available to us. Then there's the API group. The API group is a way to organize or group like objects together based on their function, things like core, which are the essential API components foundational to Kubernetes workloads, so things like Pods we'll find in the core API group. There's apps which includes the deployment object, which are used to build and model applications in our clusters. And then finally, the storage API object group, which is used to model storage inside of our cluster. There are many more API object groups, and we'll look at those more closely throughout this module. And then finally, the API is versioned. The API version defines the version schema of this representation of a particular API object in the API server. Again, we're going to look at each one of these individually in the upcoming slides. Okay, so with API groups and versioning under our belt, let's look at some of the core API objects that we'll actually use to model systems in our Kubernetes cluster, and first up is Pods. These are used to deploy our container‑based applications in Kubernetes. Then there's deployments, which allow us to declaratively deploy applications in our cluster and control our applications in terms of the container image or version of our applications and also scale our application if needed. There's services that provide persistent access point and load balancing services for our applications running on Pods. And then finally, persistent volumes. These are used to provide persistent storage to our container‑based applications. Now, in this course, we're going to have a module dedicated to Pods and instantiating workloads based on Pods. In subsequent courses, we'll have modules dedicated to each one of these fundamental resources, deployments, services, and persistent volumes. And I do want to mention that this isn't an exhaustive list of API objects. These are just the key players that we're going to use to build fundamental workloads and deploy in our Kubernetes cluster. So, now we have to discuss how we actually work with Kubernetes objects. We know what they are. Well, how do we get Kubernetes to do the things that we want it to do? And the first technique you can use is imperative configuration. When using imperative configuration, you're generally going to be executing a series of commands at the command line, operating on one object at a time, effecting the change that we want directly on our cluster. But there's a better way. We're going to use declarative configuration, and when we use declarative configuration, and this is really one of the core principles behind Kubernetes, we're going to define the desired state of our application in code using manifests. When we use manifests, we'll define our configurations in code using languages like YAML or JSON to represent the API objects that we want to feed into our API server. And then when we're ready, we'll take that code that we implemented in our manifest and use a command like kubectl apply to feed that information into our API server to effect that change of our desired state upon our Kubernetes cluster.

### Defining a Basic Pod Manifest

So far in this course we've gone through what API objects are. And we've gone through some of the fundamental objects used in Kubernetes and learned how to work with objects and manifests. Let's take a minute to look at how we write the code to implement a basic workload in Kubernetes in a manifest, and in this case we're going to create a Pod together. And so, the first line of every manifest is going to look like this. We're going to specify an API version. We briefly introduced the API versioning a bit ago, and we're going to look at this topic in much more detail later in this module. In this case, for this resource, we're going to be using the API version v1. Next up is kind. This is the API object that we want to instantiate with this manifest. In this case, we're going to create a Pod. Next up is metadata. We'll need to give our API resource a name, and in this case we're going to create a Pod so I'm going to name this resource nginx‑pod. Now we get into the technical specification or the spec part of our manifest. And since we're creating a Pod, we're going to need to define a container. And our container's going to need a name, in this case it's going to be nginx, and we're going to use the nginx image. And so once we take this information, and we can save it into a file, we could take that file and feed it into our API server with this command, kubectl apply ‑f nginx.yaml, nginx.yaml being the manifest or the text file that represents the code that we just typed at the top here. Now these four elements are required for any Kubernetes manifest, the API version, the kind, the metadata, and the spec. Now how do I know which fields or elements I need to fill out that spec part of the manifest there? Well, that's where the documentation comes in. At this link here, you'll find the documentation for the Kubernetes API objects that are available. And in addition to that, in our upcoming demonstration we'll look at using kubectl at the command line to help explain to us the elements or the fields that are available in our API objects.

### Using kubectl dry-run

So when working with API objects and writing code to represent those objects, we're going to need a way to ensure our code in these YAML manifests is syntactically correct and can be processed by the API server successfully, and that's where the dry‑run parameter for kubectl comes in. There are two types of dry‑run that we can use, and let's talk about server‑side dry‑run first. When making an API request that creates or changes resources in your cluster like kubectl create or kubectl apply, you can add the dry‑run parameter to the request, and the request submitted will be processed by the API server as if it was a typical request, but with one fundamental difference, the API request will not be persisted to storage in the cluster. Using dry‑run enables you to know if the request you're submitting is properly formatted and is able to be processed successfully by the API server, or if it is not. If there is an issue, a dry‑run will tell you what the issue is for the resource that you're trying to create or change, so for example, if an object already exists or if you had a syntax error in your manifest. On the client side, dry‑run will print the object that you want to create or change to stdout without sending the object to the API server. Client‑side dry‑run is useful to validate if a manifest is syntactically correct, but since the request is not getting sent to the API server, it is possible that the object creation or change could fail for another reason when it's submitted to the API server for real. The primary way that I use dry‑run client is to generate syntactically‑correct YAML manifests for objects. You can combine dry‑run client with imperative commands like kubectl create to output syntactically‑correct YAML manifests for objects like deployments, Pods, and others. And once you have that basic manifest, you can use that as a building block for more complex manifests. Let's look at a few examples of using kubectl dry‑run in code. First up, here is an example of server‑side dry‑run. We see kubectl apply ‑f deployment.yaml ‑‑dry‑run=server. This will send the manifest deployment.yaml to the API server for server‑side validation of the objects defined in the manifest. So this could be deployments, services, persistent volume claims, really any resources that we need to create or modify. The API server will validate if we can create or modify the resources in this manifest. Next, we see a similar example, but this time on the client side, and this will validate the syntax of the objects defined in the manifest, but will not send the manifest to the API server. Now, let's look at how we can combine dry‑run with imperative commands to generate syntactically‑correct YAML manifests. Using the command kubectl create deployment nginx, and then specifying the image's nginx, if we combine that with a dry‑run =client and the ‑o yaml output modifier, this will output a YAML representation of the object defined in the command, in this case here it's going be a deployment, and it'll write that YAML manifest to stdout by default. This is a fantastic way to quickly generate syntactically‑correct YAML manifests for objects, any objects, not just deployments. This could be Pods, StatefulSets, and so on. These manifests can be used as building blocks for more complex YAML manifests. And finally, you can take that command, and then you can combine it with I/O redirection. So here we see kubectl create deployment nginx, specifying the nginx image, add in dry‑run client ‑o yaml, and then redirect the output of that to deployment.new.yaml, and then we have a file containing the syntactically‑correct YAML for that deployment.

### Using with kubectl diff

Now, let's say we want to make a change to a resource that's deployed in our cluster, and before we do that we want to know what the difference is between the current state of the resource running in the cluster and what's defined in a manifest that we want to deploy containing changes to that resource. That's where kubectl diff comes in. Kubectl diff is used to generate the differences between resources running in your cluster and resources defined in a manifest or ones passed in via stdin. When executed, it will output the differences to stdout, and so you can very easily determine what's going to change when you apply that manifest to your cluster. At the command line, you will use kubectl diff most commonly with the ‑f parameter and read in a manifest. In this example here, it's newdeployment.yaml. For the resources defined in the manifest, it will compare those resources with the same resources that are currently running in the cluster and output the differences to stdout. For example, if this was a deployment in the manifest here, diff will go and inspect that same deployment and compare it with the deployment defined in the manifest and let you know what's different. And so perhaps it's a container image or the number of replicas that are the differences, and those individual differences are what will be printed to stdout.

### Lab Environment Overview

Before we kick off our first demonstration in this course, let's review the lab environment. We initially constructed this lab in the course, Kubernetes Installation and Configuration Fundamentals. Now our virtual machines are going to be the same. They're going to be Ubuntu 22.04, VMware Fusion VMs with 2 vCPUs, 2 GB of RAM, and 100 GB of disk space with the swap disabled, as required by the kubelet. For this lab, you can use any underlying hypervisor that you want. I just so happen to be using VMware Fusion. Now, to take DNS out of the equation in our lab, I have hostnames set for each server and specified in a host file on each of the virtual machines in the lab. As for the virtual machines that make up the lab, we're going to have one control plane node, c1‑cp1, where we'll be driving all of our demonstrations from. We'll SSH into c1‑cp1 and use kubectl locally on that server. In addition to the control plane node, we're going to have three nodes in our lab, c1‑node1, c1‑node2, and c1‑node3, and they're all going to be statically IP addressed with the addresses shown here on the screen. Now if you need help constructing this lab, head over to my course, Kubernetes Installation and Configuration Fundamentals, and you'll get bootstrapped with this configuration. I also want to call out that c1‑cp1 used to be called c1‑master1 in this series of courses. We have since renamed it to c1‑cp1 to keep up with current terminology in Kubernetes. So if you see c1‑master1 in a demo, know that it's functionally the same as c1‑cp1.

### Demo: API Server Discovery, Listing Resources, Using kubectl explain, and Creating Objects

So here we are in our first demo, and we're going to get things started with API Server Discovery. We're going to learn how to locate our API server and use the appropriate cluster context to log into our Kubernetes cluster. Then with that, we'll ask Kubernetes, Hey, Kubernetes, I need you to give me a listing of available API resources that you know about, basically the things that I can use to construct workload or objects in my Kubernetes cluster. Once we have that listing, we're going to use a tool, kubectl explain, to get a little bit further into the implementation details of the individual objects that we want to work with. And then once we have that information, we'll take that and define an object in YAML. We'll then send that manifest into our API server to create that workload in our Kubernetes cluster. Then to wrap it up, we'll use kubectl dry‑run to validate and generate some YAML manifests and then use kubectl diff to compare a manifest with a resource that's running in the cluster. So here we are logged into c1‑cp1, and let's get started with our first demo, API Discovery. Now the first thing that we're going to do is make sure that we're pointed at the correct cluster by setting our cluster context, and we can do that by first getting a listing of the configured cluster context on our local system here. We can do that with the command kubectl config get‑contexts. When I run this code, what this is going to do is output any of the cluster context that I have configured on my local kubeconfig file. In the output here, I have two cluster contexts defined. Let's walk through both of those, the first being CSCluster, which is the Azure Kubernetes service cluster that we created together in the course, Kubernetes Installation and Configuration Fundamentals. The second line of output we have is a kubernetes‑admin@kubernetes. That cluster context is the local kubeadm‑based cluster that we built together on virtual machines. In the left‑hand column there, we see an asterisk in the CURRENT column next to that cluster, meaning that this is our current cluster context. If we needed to change our cluster context, we can use the command here on line 11 to do just that‑‑kubectl config use‑context and then specify the context name, and here we have kubernetes‑admin@kubernetes. Let's go ahead and run that code together, and we can see the output here. If we needed to switch our cluster context, the output would say Switched to cluster context "kubernetes‑admin@kubernetes." Now that we know that we're pointed at the correct cluster, let's use the command kubectl cluster‑info to get some information about our API server. And in the output at the bottom, we can see the network location of that API server. So let's walk through that together too. Kubernetes control plane is running at https://172.16.94.10 on port 6443. So now we know that we're pointed at our local on‑premises Kubernetes cluster with that API server information there. And so now that we know we're pointed at the right cluster, let's use the command kubectl api‑resources to get a listing of the API resources available for us in this cluster. We'll go ahead and run this code together, and in the output, we'll get a listing of all the different API resources available to us to build and construct and configure workloads in our cluster. And so let's walk through this output at the bottom. On the left here, we have NAME, which is the name of the app resource. We have SHORTNAMES, which is an alias that we can use to address that same type of object but with a shorter name. We then see the APIVERSION, so everything on the screen here is a v1. Now we haven't touched on what a namespace is yet, but a namespace is a way to group resources inside of a Kubernetes cluster. And we're going to cover that in great detail in the next module. But here we have a column that describes if this particular API resource can be in a namespace or not. And then, finally, we have the KIND column. The kind is a string value representing the REST resource for this object. And so in the output, we see some things that we're familiar with from a workload standpoint. We see nodes and Pods, and if I go down a little bit further, we'll see things like deployments and replicasets. So API objects used to construct and configure workloads in our cluster. And so now that we know the different API resources available to us, maybe we need to dig a little bit further and get some more information about those different API resources to help us understand how we can build a workload with those particular API resource types. And we can use the command kubectl explain to help us with that. And so on line 24 here, I have kubectl explain pods. And what that's going to do is give me some detailed information about that particular resource, and this could be any resource that's available to us in our cluster, things like deployments, replicasets, and so on. But we're going to look at Pods more closely in this demo. And so when I run kubectl explain pods, what we're going to get is some deep‑dive information about that particular resource. And so here we see KIND is Pod, VERSION is v1. We also see a DESCRIPTION: Pod is a collection of containers that can be run on a host. So telling us exactly what this type of resource does. Now going down a little bit further in the FIELDS section, we see what it takes to construct this API resource in our cluster. So we see things like apiVersion, kind, metadata. And if I go down a little bit further, we'll see spec. And so with that then, we can start understanding what it takes to build the YAML manifest that describes this type of object that we want to deploy. Now I do want to call out that spec is a type object, and so what that means is we can go even further into that object with kubectl explain. And so let's look at how to do that. On line 28, I have kubectl explain pod.spec. Let's go ahead and run that code, and now what we have is the information to help us color in those details about what's needed for a PodSpec. And so if you're working with the workload object or any API resource, you can use this technique to kind of discover what it takes for all the various configuration elements that you might not have memorized. But you certainly could go look those up on the API references online on the web, but they're all available to you at the command line here at your fingertips. And so let's look at this output. So here we see KIND is Pod, VERSION is v1, and the RESOURCE is spec. In the DESCRIPTION, it says, The specification of the desired behavior for the Pod. And so this is where we're defining how a Pod is going to function. So in the FIELDS at the bottom, we'll see things that we're probably familiar with in a PodSpec, specifically containers. And so here we see containers is a type object, and it's also required. And so that's going to be something that we have to put in every single PodSpec that we write. And so let's break out of this output. And since that's an object, we can use kubectl explain to dive even further and get more documentation about that particular facet or field of the API resource. So kubectl explain pod.spec.containers, and now we get the description and the fields of pod.spec.containers. Let's go down to a field that might be familiar to us. Go down to image. And so here we see image is where we'd specify the container image that we want to launch. Here it says Docker image name. So let's go ahead and break out of this output now, get back to our terminal, and look at a Pod manifest that I already have written that is available to you in the course's downloads. And so now you can kind of make the connections between what we just saw in kubectl explain and what we see in a Pod manifest‑‑apiVersion: v1, kind: Pod, metadata: we give it a name, hello‑world. We then have the spec, the technical implementation details of how this Pod behaves. In this case, it's just going to run a container, one container named hello‑world, loading up the image from the Google container registry for our hello‑app application that we've been using in our demonstrations in this series of courses. And so jumping back to our driver script, let's go ahead and create that Pod with kubectl apply ‑f pod.yaml, and we go ahead and create that Pod. And so that technique of discovering API objects to understand how to construct them and then using that to help us construct the Pod manifest is going to be a common technique that you need in building your workloads inside of Kubernetes. Now before we move forward into the next demo, let's make sure that that Pod got created successfully with kubectl get pods. And at the bottom here, we can see in the output that that Pod was certainly created, and its status is up and running. And so with that, before we move forward to the next demo, let's delete that Pod with kubectl delete pod hello‑world.

### Demo: Working with kubectl dry-run

Now moving forward, let's learn how we can use kubectl dry‑run for both a server‑side and client‑side validation and also to use it to help us generate syntactically correct YAML at the command line. And so the first thing we're going to do is we're going to use server‑side validation for a deployment manifest. And so let's check out how to do that with the code on Line 50. And so here we have a kubectl apply ‑f deployment.yaml and I specify ‑‑dry‑run=server. And so when I run this code, what that's going to do is take that deployment manifest and send it into the API server for validation, it's going to check both is it syntactically correct and can it actually create those resources in the cluster for me. And here in the output, we see deployment.apps/hello‑world created, but the difference here than for normal output as we see in parentheses (server dry run), and so that tells us that we can successfully create or change the resources defined in that manifest. If I go ahead and look at the deployments that I have available in my cluster with kubectl get deployments, we can see that well, we didn't actually create that deployment, it just validate it if it was syntactically correct and if it could create that in a cluster because we specified dry‑run=server. So let's move forward and try some client‑side validation. And so, very similarly we say kubectl apply ‑f deployment.yaml, so we're reusing that same manifest, but this time instead of saying ‑‑dry‑run=server, we're going to do client‑side validation and say ‑‑dry‑run=client, run that code there, and we can see similar output, but slightly different. We see deployment.apps/hello‑world created, and then just (dry run) in parentheses in contrast to what we saw a second ago where it said server dry run. And so with that, we saw both server and client‑side validation for correct manifests. Let's see what happens when I feed in a manifest with an error. And so on Line 62 here, I have kubectl apply ‑f and then I'm specifying a different manifest deployment‑error.yaml, and they're specifying ‑‑dry‑run=client. And so when I run that code and do client‑side validation, well, it's going to fail and we can see the error at the bottom. And so in the bottom here, we have error validating deployment‑error.yaml error validating the data. Scan a little bit further over on the right, we see unknown field replica in our deployments spec. And so, what that's telling us is well, that there is an error in that field in our deployment spec. And so, let's jump over to a deployment‑error.yaml and what we have here is on Line 8, well, I've incorrectly put the field in as a replica instead of replicas plural so that should have an s there on the end to make that syntactically correct, and that's exactly what the error told us in the output at the bottom here is that there was an error unknown field replica. And so, using client‑side validation with dry run is a good way to sift out configuration errors in your deployment manifests quickly. Now one of my favorite ways to use dry run is to combine it with imperative commands to generate syntactically correct YAML, and so let's check out how we can do that. And so first up, I want to show you that we can use kubectl create deployment nginx to create a deployment and specify in our image name and then adding the parameter dry‑run=client. And so when I run this code here, that's going to validate that well imperative configuration command. We can combine dry‑run=client with imperative commands and then also with an output modifier so that it can generate YAML for us and here we see the code to do just that on Line 70, so the same code as above, but we add in ‑o yaml. And so on Line 70, I have kubectl create deployment nginx and then we're specifying the image nginx dry‑run=client, and then ‑o yaml and then I'm going to pipe that into more so it doesn't run off the screen. And at the bottom here, you can see we've generated the YAML for a simple deployment, and we can use this either as a foundation for a more complex deployment or just to test things out to quickly generate manifests for deployments in our cluster. And so, I do want to point out though that that's not just for deployments, that could be for stateful sets, that could be services for whatever. Here, I have an example of a pod, and so we're going to use the same technique to generate a manifest for a bare pod, kubectl run pod and the name is going to be nginx‑pod for the image nginx and then dry‑run=client ‑o yaml pipe that into more, and now we have a manifest that describes that pod with that nginx image for us to use or to build upon if we need to do so. Now we can take this technique of dry‑run=client and ‑o yaml with that output modifier and use I/O redirection to store that generated YAML in a file and that's what we're doing just on Line 78 here, so kubectl create deployment nginx, nginx is the image again, and then specifying dry‑run=client ‑o yaml and then redirecting all of that output into a file named deployment‑generated.yaml. And so, let's run that code and then I'll use the more command to output that to console and here we can see that deployment manifest. Let's break out of this output at the bottom and we can use that manifest right away to ship that into the API server to actually create those resources and that's what we're doing here on Line 83, kubectl apply ‑f deployment‑generated.yaml, run that code, and at the bottom here, we can see deployment.apps/nginx is created. And so using that technique of kubectl create and then combining that with whatever resource it is that you want to create and then dry‑run=client and then the output modifier ‑o yaml, you can very quickly and easily generate syntactically correct manifest for the resources that you want to generate. And so let's clean up from this demo and use kubectl delete to delete the manifest that we just created, deployment‑generated.yaml, and that's going to delete our deployment as indicated in the output at the bottom.

### Demo: Working with kubectl diff

Now for our final demo in this series of demos, we're going to look at working with kubectl diff. The first thing that I'm going to do is create a simple deployment with 4 replicas in our cluster, and here I have the code on line 94 to do just that, kubectl apply ‑f deployment.yaml. That's going to create a simple four replica deployment in our cluster. Looking at the deployment manifest for that, we can see that it is a deployment that has four replicas, and it's running the container image version 1.0, as we see on line 19 here, hello‑app with the tag of 1.0. Let's jump back to our driver script here. I now have a new deployment manifest that I have already written called deployment‑new.yaml, and in that manifest, I've made some changes. And let's use kubectl diff to help us understand what the differences are between the currently running object in our cluster and the deployment manifest that I want to deploy into the cluster. I want to see what the exact changes are between what's running and where we're about to go to with this new deployment manifest. And on line 98, we can do just that, kubectl diff ‑f deployment‑new.yaml, and I'll pipe that output into more. Let's skip past this metadata at the top here and jump down into the deployment and pod template spec at the bottom here, which is where the changes that are interesting to us are located. And so in the pod spec, we see replicas 4 and a minus sign, so that's the difference, and then plus is the change, replicas 5. So we went from 4 to 5 as indicated here in the output. Down to the bottom in the pod template spec, we see containers, we see the changed line with a ‑ sign. So image, there's the version 1.0 of our application, and then the line that we're adding or changing to right below that. So we see that the image is version 2.0 of our application. And so with kubectl diff, you can very quickly and easily compare resources that are currently running in a cluster with the resources defined in a manifest and highlight the differences between the two. And so that's what we did here with kubectl diff. Let's wrap up this demo, and use kubectl delete ‑ f deployment to clean up and to delete that deployment.

### API Groups and API Versioning

So it's time for our closer look at API groups. API groups enable better organization of resources in the Kubernetes API. There are two high‑level organization methods for API groups in Kubernetes. The first is the core API, or legacy API group. These are the Kubernetes objects used to build the most fundamental workloads, and here you'll find things like Pods. Back when Kubernetes was first developed, there was no concept of API groups, and these objects were simply defined in the API. Officially called the core API group, you'll often see it referred to as the legacy group or legacy API group as well. Now, as Kubernetes developed, and the need for groups became more apparent, newer objects were placed into named API groups. A common example of a named API group is the storage API group. Since the Kubernetes API is a RESTful HTTP‑based API, resources in Kubernetes are referenced via a URL or URL path. For named API groups, the API group name becomes part of that URL path. That's not the case for the core API, or legacy API objects, which are just part of the base URL for their API version. We'll look more closely at URL paths later in this module. So now let's roll through some examples of resources in both the core and named API groups, and let's kick it off with the core API group. In here, you'll find things like Pods, Nodes, Namespaces, PersistentVolumes, and PersistentVolumeClaims. Again, these are the most fundamental elements in the Kubernetes API, and have been around since its inception. So now let's look at some named API groups and example objects from each group. The first API group that we're going to look at is the apps API group, and in here you'll find a common resource like the Deployment object. Then there's the storage.k8s.io named API group where you'll find the StorageClass. And finally, rbac.authorization.k8s.io, where you'll find things like the Role or ClusterRole objects used to assert some security or role‑based access control in our cluster. These are some examples of some very commonly used API objects. If you want to see a bigger list, then go ahead and check out this link here. Moving on in our deep dive into the API server, next up is api versioning, and the role that it plays in helping us build stable systems in our Kubernetes clusters. The Kubernetes API is versioned at the API level. This way we know what version of the API we're interacting with when we define resources in code. There could be multiple versions of the Kubernetes API on a server at any point in time, and this helps us by providing stability for our existing implementations. If we're using declarative deployments, and we're building systems and code using objects to model our system, like Kubernetes API objects, well, those objects might need to change over time, and Kubernetes provides us a mechanism to help us absorb that change when we're ready, and that's API versions. This allows us to specify which version of the API we want to interact with for those objects. API versioning enables both backwards compatibility and also forward change, so that we can specify the API version of the object that we want to work with in our code, and we've done this so far together when we've declared an API version in our YAML manifest. Now, as an API version develops, it moves between different phases of the Kubernetes development process, starting off in Alpha, moving on to Beta, and then onto Stable as objects and API versions become more tested and stable. Now, I do want to call out that there's not a direct relation between the API server version and the system‑level released versions of Kubernetes. At the time of this recording, Kubernetes is on system release 1.21. Newer versions of the API server and Kubernetes system software will be released, but they're not in lockstep with each other from a development standpoint, nor from a release standpoint. So let's look more closely at API versioning and those three phases of development that I introduced just a second ago. We're going to look more closely at Alpha, or experimental; Beta, or pre‑release; and then Stable, or general availability code. When working with the Alpha releases, you're going to find it's going to be versioned like this, where you see V1alpha1, or Alpha specifying that this is part of an Alpha release. This is going to be early‑release, bleeding edge stuff, brand new code coming out of the project team. It's going to be disabled by default in your API server, so you're going to have to go proactively enable this version of the API on your API server to consume objects from this API version. The code that's released in Alpha releases is for testing only. This is brand‑new stuff coming right out of the development lifecycle, and one of the things you're going to need to look out for when using Alpha release code is breaking changes. There's no guarantees that an object between versions of Alpha releases will look the same, so that API surface area might change between different versions of the API version in the Alpha phase of development. Now, as code moves along and gets more mature and more tested, it will move into the Beta, or pre‑release phase. Here you'll see a version like this, where you see V1beta1, indicating that it's part of the Beta release cycle. This is more thoroughly tested code and is considered safe, but it's still encouraged that you test this code in your systems before using it in production. There's some more stable guarantees around API object surface area, so different versions of the API server at the Beta release level have a less likely probability of changing between different versions. Now, one of the things that the project team encourages is feedback. They want to understand what your experiences are with these objects at this release level, and go over to GitHub and give them that feedback so that they can help make the objects that are released in particular API versions better and more production ready. Now, moving along through the lifecycle, we'll get up to Stable, or GA. This is going to be indicated with a versioning schema that looks like this. Here we see v1. The code at this level will have backwards compatibility between different versions. There are some policies around deprecation of objects and how long they have to live in an API version before they go away and become deprecated. This is considered production ready code, so when you're building your production systems, start here.

### Demo: API Object Discovery - API Groups and Versions

Alright, time for a demo where we're going to look at API object discovery. Specifically, we're going to look at examining API groups and looking closely at specific API versions of objects available to us in our Kubernetes API. So just like in our previous demonstration, we're going to start off with some API discovery, but we're going to go down a different path this time focusing on API groups. And so, let's get a listing of all the API resources that are available to us on our API server and we can do that again with kubectl API resources. I'm going to pipe that output into more so that it doesn't run on the screen. And so, in the output at the bottom here, we can see a collection of resources that are available to us. And so in the first page of the output. we see that all of the resources are in the API version V1, this means they're in the core API group and all have an API version of V1. If I go down a little bit further in the output, we can see other resources that are part of different API groups, and in fact, different API versions and so let's zoom in on a couple of those. The first collection of API objects that I want to call out are the ones in the apps API group, so there we see daemon sets, deployments, replica sets, and stateful sets, and so those are workload objects that we use to well build applications in our Kubernetes clusters. Those resources are all on an API version of V1, and so here we see apps for the API group and V1 for the API version. Going down a little bit further in the output here, we can see that we have a resource that's not a V1 API version resource, and so let's look at one of those. There, we see cronjobs, which is in the API group batch and has an API version of v1beta1 because it's not quite graduated onto v1 yet. At some time in the future, you can expect cronjob will graduate on the v1, and so if you're trying to reproduce this demo and you see it's now on v1, that's okay. There is likely some other alpha or beta version resources available in the output here. So let's break out of this output here and move forward into the next part of our demo. Now, if I want to get a listing of the available resources in a specific API group, such as apps, I can use kubectl API resources to do that. When I specify the API groups parameter and I say api‑groups=apps, what that's going to do is give me a listing of the api‑resources that are in that API group on the server. And so this is a way for me to help discover and to navigate which API resources are available to me on my cluster that are part of specific API groups. Now let's say that I wanted to work with a specific version of an object, maybe I'm writing a manifest and I want to use an exact version of an object and while one of the ways that I can go about doing that when I'm writing a manifest as well, I'll specify the API version in the manifest, but how am I going to know the exact implementation details of that specific API version for that object and well we can use kubectl explain to help us do that. And so here on Line 13, what I'm doing is I'm specifying that I want an exact version of a deployment. So here we see, kubectl explain deployment, and for the parameter API version, I'm specifying apps/v1, and so this will explain that exact version of a deployment. Now, up until recently, deployments were actually in the apps/v1beta1 and then graduated onto apps/v1. And so if there was different versions of deployments available to us in our API server, we could specify that here at the command line, but as is of this recording, there is only one version of a deployment available to us in our API server. Now if I want to go and I want to look at all the supported API versions and groups that are on my API server, I can use the command, kubectl api‑versions to do that, and I'm going to pipe the output of this to sort and then to more to make it a little more readable for us. And so in the output here, we can see all of the different API groups available to us on our API server and all of the various versions of those that are currently available on our current API server. And so going to there, we can see lots of different objects in various stages of development. So there we see again batch/v1, batch/v1beta1.

### Anatomy of an API Request: API Verbs and Special API Requests

So let's begin looking at the anatomy of an API request. When we're working at the command line with a tool like kubectl, we enter a command, and it'll convert that request from YAML into JSON. And it will then, based on the operation that we want to perform, submit that request into the API server. These requests will be based on RESTful API actions, things like GET, POST, and DELETE. Now since our API server is a RESTful HTTP web application, we'll need to submit our request and specify a URL resource location or API path. Encoded in this path will be our API group, whether it's core or named, the version of the API that we want to work with, and also the object type itself. Now when we submit our request, and if it's a valid request that changes the state of our system, the API server will commit that to the cluster store with the WRITE operation and send back a response with an HTTP response code that indicates what happened during that operation. If our request was a query for information, that'll be read from the cluster store, and that information will be sent back to the client as well with the appropriate response code. In this course, we're primarily going to focus on the API request, API paths, and the responses sent back from the server to the client. We'll look more closely at the cluster store in an upcoming course. The Kubernetes API is a client/server architecture where clients submit requests to the server. And primarily we've been using kubectl to do this work, but really any HTTP client that respects the protocol set forth by the API server and the API itself will be able to communicate with the API server and submit requests. So this gives us the ability, if needed, to build custom tooling to interact with your Kubernetes cluster. We're going to look at some examples in our upcoming demos where we'll do some investigation of our API server with the command‑line web browser curl. As long as we submit a well‑formed request with curl, we can get the API server to perform the operations requested. Now that leads us to, What does a valid API request look like? Well, we already know that our API server is an HTTP‑based RESTful API. What this means is we'll need to define requests in terms of HTTP verbs like GET, POST, and DELETE, and so on, and also specify a resource location, essentially a full path or a URL for the object that we want to interact with. You take that verb and you add in the resource location, and then you have a valid request that you could submit into your API server. Once you submit that request into the API server, the server will respond with the response code that indicates the outcome of your request. Was it successful? Was there an error? Did you fail to authenticate properly, and so on? Let's look at some of the more common API verbs available in the Kubernetes API. First up is GET. GET gets the data for a specified resource. Now we've used this verb many, many times when we did kubectl get and then specified some resource, like Pods or nodes or deployments. What happens behind the scenes is kubectl converts this into the necessary app request to submit this verb into the API server and retrieve that data for us. There's also a POST operation, which is used to create a resource, DELETE for deleting a resource, and PUT where we can create a resource if it doesn't exist or update an entire resource if it does exist in our cluster. And then, finally, PATCH if we need to modify a specified field of a particular resource when we compare this with PUT, which is going to update the whole resource or the whole object. In this case, PATCH will update an individual field or an individual element of a particular API resource. In addition to the API verbs that we just looked at, there are some special API requests that can be made of the API server that facilitate cluster operations and workload management in our Kubernetes cluster. And first up is LOG. LOG is an API request that retrieves logs from a container in a Pod, and it'll stream that output back to the requester. Most commonly, we'll use kubectl combined with the LOG parameter to request this information from a particular Pod. Then there's EXEC. We can use the EXEC verb to execute a command in a container inside of a Pod and get that output back to the requester. This is also commonly used at the command line with kubectl exec. And, finally, there's WATCH. WATCH is an API request that allows us to get change notifications on a resource with streaming output. And let's talk about what that really means behind the scenes. Each resource in Kubernetes has a resource version associated with it. And so when we instantiate a WATCH on an object or a resource in our cluster, when that version is changed on the watch, notifications will be sent to the client that are watching that particular version. Now a common use case for this is when you specify the ‑‑watch flag on a kubectl request so that you can get notifications on changes in state for objects. Things like deployment rollouts or Pod creations are common use cases for using watches, adding that ‑‑watch parameter to our kubectl command.

### API Resource Location (API Paths) and API Response Codes

So, we've been focusing on the request, specifically the API verbs. Now, the other part of the equation to make a valid API request is specifying the API resource location, or the API paths. Now API paths are really thrown into two different buckets, like API groups. API paths are split across API groups. And so first we'll see the core API group, and then next we'll look at the named API group. And so in the core API group, we'll find that our URL path, or our API paths, the resource locations look like this. We'll see http://apiserver:port/, followed by the string API and then the version of the API that we want to work with, whether they be v1, v1beta1, and so on, and then slash the resource type that we want to work with, is it pods, is it persistent volumes, whatever it is. Now, if the resource that we're working with is in a namespace, we're going to get a slightly different URL representation. So here we see http://apiserver:port/, then it's the string API followed by the API version, again v1beta1 or v1, whatever it is. After that, we have the string namespaces followed by the actual namespace name. Inside of that namespace, we'll see the resource type, whether it be a pod or whatever it is, as part of the core API. And then if we're addressing an individual resource then it's going to be that resource's name. Now when we're looking at resources that are in an API group, like a named API group like storage or RBAC and so on, we're going to get a slightly different URL representation. Here we see from left to right, http://apiserver:port and then the string APIs, plural, followed by the group name. And that would be something like storage or app or whatever it is for the particular API group that we're working with. Next we'll see the API version. Again, that could be v1, v1beta1, and so on, followed by the resource type. If it's a namespaced resource, again we'll get a slightly different representation, but similar to what we worked with above here. We see http://apiserver:port, again followed by the string APIs, the API group name, the version of the API server that we're working with, and the string namespaces, followed by the namespace name, the resource type, and then the resource name. Now this might seem a little abstract, but don't worry I kind of wanted to get this in front of you. In the presentation portion of the course, we're going to do lots and lots of demos together where we'll use kubectl to make various requests of resources in our cluster. And so we'll look at different API paths and also the various verbs that we'll use in those requests. Once we've submitted an API request into the API server with the combination of API path and the verb or operation that we want to execute, then the API server's going to process that request that we made and then send us a response back based on what happened inside of the API server. So let's look at common API responses that we'll get back from the server. And we're going to group them into three classes, the 200 for success, 400 for client errors, and 500 for server errors. Now, if I submit a request and that request is processed and everything goes okay, well then we'll get back a response code of 200, which translates to OK. Now if I submitted a request with a POST operation into Kubernetes and it responds back with a 201 Created, that tells me that the resource that I requested was successfully created. Now there are some scenarios where the request that we're submitting is going to be processed asynchronously, meaning Kubernetes is going to go and deal with that in the background. And in those scenarios, we'll get back a 202 Accepted response telling us that the response was successfully accepted. Now let's jump into the client errors bucket here. In client errors we'll see commonly 401, or Unauthorized. Basically, I did not authenticate to the server correctly. There's also 403 Access Denied. We're going to see this in scenarios where the user has authenticated to the web server, but they don't have access to the API resource or API path that they're trying to interact with, where they can't use that particular operation on that particular API path. This is going to come from the security policies applied to both the user and the resource in the cluster. And then finally, there's 404, or resource Not Found. That's going to pop up when I try to interact with an API resource at a particular API path that's just not there. So very similar to what you see in regular web servers when you try to access a resource that's just not available on the web server. And then finally server errors, and those most commonly roll up into one error, 500 or Internal Server Error. Now, there are many more response codes available inside of Kubernetes. These are kind of the primary ones that we're going to see in everyday workings with our API server.

### Anatomy of an API Request - A Closer Look

Now let's walk through the lifecycle of an API request from client to server, and the intermediate stages that happen in between those. And so first up is connection, then we go to authentication, then we look at authorization, and then finally, admission control. Now let's look at each one of these phases in much more detail. In the connection phase, we're basically trying to figure out, can we make a connection to the API server? And that's certainly going to be HTTP over TCP, but most commonly we'll see these connections as TLS‑encrypted HTTP sessions or HTTPS. Then we move into the authentication phase, we're basically logging in. We're trying to determine, are you a valid user? The user is going to be validated via an authentication plugin, which is modular, used to authenticate users via various authentication types such as certificates, passwords, tokens, and other various authentication plugins. If you're unable to authenticate to the server, you'll get a bad authentication request, and that's going to be visible in your HTTP response with a 401 error. Next up, in the authorization phase, we're trying to figure out can you, the user, perform the requested action on the resource that you're trying to operate on? In that case, it's going to be a combination of, can the user execute the API verb on the resource or API path that they want to interact with? In Kubernetes, actions are denied by default, and roles are established to allow users to access and perform actions on the object or resources that they have permissions to operate on. If you don't have permissions to perform the action that you want on the resource that you want to operate on, the API server will respond with a 403 error. And the final phase of our API request here is admission control. Admission control gives us administrative control over an API request with an admission controller. An admission controller is a piece of code that intercepts requests to the API server prior to persisting that object to the cluster store. When an object is being created, deleted or updated, that request is passed through the admission controller, which can modify or reject the request. An example of a modification is adding default values to an object, overriding object values with some configuration standards for our clusters or maybe asserting resource quotas or any configurable attribute of an object. And then the final part of admission control is validation. This is the part of admission control where the object being requested is validated using the validation routines for the objects that we want to work with. Basically, is this a well formed request for this particular API object? Now, all of these steps happen prior to our object being persisted to our cluster store, etcd.

### Demo: Anatomy of an API Request

Now let's get into a demo and look at the anatomy of an API request. So we're going to look very deeply at what happens when we make requests from the client over to our API server. We're going to look at some special API requests, both watch, exec, and log, and look at authentication failures and missing resources, so we can see what to look for when things go wrong. Then we're going to wrap it up with a very close look at the API interactions between client and server when we create some objects, specifically deployments. All right, so here we are in VS Code, and we're going to kick off our demonstration starting with the anatomy of an API request. And the very first thing that we're going to do is create a Pod together, and I have some code here for that with kubectl apply ‑f pod.yaml, and there is just a simple Pod creation of the hello‑world Pod that we've used in previous demonstrations. So here we can see pod/hello‑world created. Let's go ahead and double check that, and use kubectl get pod hello‑world to make sure that our Pod is up and running, and it certainly is. So there we can see STATUS Running. Now, when working with kubectl, we can use the ‑v option to increase the verbosity of our request, and, well, what does that really mean? Kubectl is a client, and the API server is, well, our RESTful API server for our Kubernete cluster, and we can increase the verbosity or the output from the kubectl command, so we can get more insight into the interactions between our client and the server. And the first one that we're going to start off with is we're going to increase the verbosity with this ‑v parameter here to level 6, and that's going to give us a lot of information about the transaction between the client and server. And so let's go and do that together and review the output. So the code here you can see, I have kubectl get pod hello‑world ‑v 6. And so let's go ahead and run that code together. Now at the bottom here, we've got a couple lines of output, and we're going to go through each of those together. Now the first one is where it says config loaded from file, that's just reading in our kube‑config file for us to authenticate against the API server, or loading our certificates up, finding the API server, things like that. That next line there in the middle, we can see an API verb GET. That API verb is the operation that we want to perform against the API server, and that's the next part of this line of output here. We see https://172.16.94.10 on port 6443. That's the location of our API server. Now, the rest of that line there is the API path, and so we see it starts with /api, then our API version, v1, we see the string's namespaces, and then the namespace specifically is default, and so this Pod was created in the default namespace. After that, we see pods, or type of resource that we want to interact with, and then hello‑world, which is the individual Pod that we want to interact with. Now after that, we see the response code from the API server, we see 200 followed by an OK, and then the duration of the request, 9 milliseconds. And with this output, we can see the direct interactions between the client, which is kubectl, and the API server. This is the request made of the server and the response code back, right, the request consisting of the API verb, the API path, and also the response code. Now at the very bottom here, we can see the output that we're used to seeing when we use kubectl get pod where we have the name of the Pod, if it's up and running, and the number of restarts, and age, and so on. Now I'm going to scroll down a little bit in my code here and show you that we have some other verbosity levels that I want you guys to explore on your own, so you can see the information that's exchanged between kubectl or the client and the API server. And so as we increase the verbosity, we're going to get additional information and have some comments here in the code as to what you're getting as you increase the verbosity. So there we can see on line 12 and 13, when I increase the verbosity to 7, well, I'm going to get the same output of 6, but some additional information, in this case the HTTP request headers. So we'll see things like application type and the user agent that is specified in the request headers of the request sent to the server. And then as we increase to 8 and 9, gaining additional information with regards to the truncated response body, and then at level 9, the full response body. So you'll see the whole HTTP transaction and the data returned to kubectl in its non‑processed format. So let's go ahead and jump a little bit further ahead down to this batch of code here. And we're going to do some deeper‑dive API requests with the command line web browser cURL. Now, for me to use cURL against our API server, I'm going to need to authenticate that, and an easy way or a good way to authenticate against our API server is to use this command here, kubectl proxy. What kubectl proxy is going to do is make a connection from my local machine to the API server on a port, but what it's also going to do when it makes that connection is use my local kube config for kubectl to authenticate me against the API server. So now with that local connection, I can submit it to that local connection that was created, and I'm already authenticated to the API server, and so that's going to be relayed into our cluster API server, and I'm allowed to interact with that with tools like cURL. And so let's kind of walk through a demonstration so we can see that in action. So on line 23 here, I have kubectl proxy followed by an ampersand, which simply backgrounds this process, and so there we can see at the bottom here, starting a server on 127.0.0.1:8001. So that's my local proxy connection, and I'm going to make a request of that localhost address on that port, and that's going to be proxied or relayed over to the actual API server. And so let's go ahead and do that together on line 24 here. So from left to right, we see curl, which is our HTTP client, and we're going to request a particular URL. Now that URL is going to be the local kube‑proxy connection. So we see http://localhost:, or 127.0.0.1, on the port 8001. Now the rest of that API path is going to be the object that we just created together, and we got that API path when we used kubectl get pods ‑v 6, and we saw the API path in the output of that request. And so let's go ahead and walk through that API path together. We see /api, we see the API version, v1, /namespaces/default/pods/hello‑world, all the way out to that specific resource that we want to work with. And so this request is going to go and ask our API server for the JSON object at that API path. And so let's go ahead and run this code together. I'm going to pipe the output in ahead just to limit some of the output so I don't have to scroll so much because there is a lot of information coming back in this request. Now if we scroll up a little bit, we'll see some information that we're used to seeing. Here we see things like kind, and apiVersion, and metadata, right? This is the JSON representation of the object that we retrieved directly from the API server itself. And so using this technique, and we're going to build on this in later courses, this is a great troubleshooting technique when you need to interact with the objects to understand what's really going on with the state and maybe some information that's not exposed in the tooling that we have with kubectl. So let's go ahead and bring a prompt back up here, and bring our process into the foreground with fg, and then break out of that with Ctrl+C. So we just killed off our kubectl proxy.

### Demo: Special API Requests - Watch, Exec, and Log

Now let's move on into our next demo. We're going to do some of those special API operations so that's going to bring that code into view here, and start off with a watch. What a watch is going to do is watch a particular resource based on the resource version of that particular object or collection of objects in our Kubernetes cluster. And so let's go ahead and kick off a watch on pods. And so, what we're going to do here is use the command kubectl and get pods ‑‑watch and I'm going to use the verbosity level ‑v 6, I'm going to put that in the background with an ampersand and what that's going to do is watch for any information on any pod operations in the default namespace. And when I initiated that command with the ‑v 6 parameter, it gave us the verbose output of the request and let's look at that request at the bottom. There are two lines I want to call out to you here. The first one we see a get operation against the API path to all of the pods in a default namespace. And so let's walk through that line together and kind of get a feel for what that looks like. We start off with the API server, so 172.16.94.10 again on port 6443. We see /api/v1/namespaces/default. We're going to operate in a default namespace. Now we're operating on the object or resource pots. Now we see limit 500 because it's going to limit the output that it retrieves from the cluster via the API server. We see 200 OK for our response code so we know that we were able to retrieve a list of pods. Now we initiated a watch as well, and so the standard output of the command would give us our normal kubectl get pods output, but since we initiated a watch, we're going to get additional information as those resources change. And let's look at that second API request here at the bottom. So here we see a get operation again against our API server. We see the API path is /api/v1/namespaces/default/pods, that same resource location that we've been working with, but we see an additional parameter there ?resourceVersion = 2691186, and then at the next line with a little line wrap there, we see &watch=true. So any time there is an operation on a pod in a default namespace, that resource version is going to change and the output is going to be streamed back to us from kubectl get pods and so we'll be able to see things like changes in state for these particular resources, in this case, pods. Now, let's go ahead and get a new line here at the bottom, and scroll back up to the top here and use netstat ‑plant | grep kubectl to get a list of the network connections associated with the kubectl process. You can see we have a persistent connection to the API server. so we see our local client which is actually still on the master. We see 172.16.94.10:42514, we see that as our local connection to the remote connection or the API server, which is resident at 172.16.94.10:6443, so we see an established TCP connection so as something happens on pods, that connection is already built, that output is going to be streamed from the API server back to our local kubectl process that's up and running with our watch, which we background it on Line 31. So let's go ahead and do something, let's go and delete a pod and see what happens. And so with kubectl delete pods, we're going to delete the hello‑world pod we've been working with. And so, here you can see at the bottom the streaming output as the state changes for the pods in the default namespace, in our case, the hello‑world pod. So we see starting off, we see hello‑world 1/1 Terminating, that's a state change. We initiated the terminating process. And then as it is watching and that state changes, we see it goes to 0/1 which means the container shutdown and then it takes a couple more samples, and then finally, the pod is shut down and deleted, but let's go ahead and bring our pod back with kubectl apply ‑f pod.yaml and watch the creation of a pod with our kubectl get pods watch operation. It's going to run that there and we can see as it changes through the different statuses to bring our pod up and running, it starts off with pending, goes into container creating, and then shifts over into the running state. Now that actually is completed, even though we didn't get a new line because of the way that the watch works and it doesn't grab a new line for us, so if we go ahead and simply hit an Enter key right there, we'll get our prompt back. So let's go ahead scroll up a little bit further and go ahead and kill off our watch, I think we're done with that for now, so bring that into the foreground and then use Ctrl+C to break out of that. The second special API request we're going to look at today is accessing logs, and so let's go ahead and do that together. We're going to go ahead and start off with just grabbing the logs without any increased verbosity. So we say kubectl logs hello‑world and that's going to pull the logs from the container running in our hello‑world pod. And so, we can see there is one line of output here, that's the only line in the log on that container that's up and running. So we see server listening on port 8080. Now if I run that command again and increase the verbosity to the level 6, we'll see the API request is a little bit different than just a regular kubectl logs. So let's walk through the two API transactions at the bottom here. So first up, we see a get against our API server all the way out to the API path for this particular pod. So we see that it comes back with a response code of 200 OK. And so that's checking to see if that pod is actually there, and if it finds that the pod is there, it makes a second request to grab the logs from that pod. And so in this case here, we can see a second API request with the get operation against that same resource, so we see /api/v1/namespaces/default/pods, then the pod name hello‑world, but there is an additional entry on the API path, this is /log and that's the API location to retrieve the log from that particular pod. You get a response back of 200 Ok, and at the very bottom there, we see the log output from that operation server listening on port 8080. Now I'm going to combine two techniques here, we're going to combine using kubectl proxy to make a direct request of an API resource using curl again, that command line web browser, and so let's go ahead do that together. So kubectl proxy with an ampersand on the end to background it. Our proxy is still listening on our localhost address at 8001. Now I'm going to make an API request of that same API path to retrieve the log directly from the API server, not necessarily using kubectl. We're using curl, the command line web browser. And so let's look at this command from left to right. We see curl, then the location of our local kube proxy, which is relaying that information or that request into our API server. So that's a localhost:8001 API path out to the resources /api/v1/namespaces/default/pods/hello‑world, and then finally, log. And so when we execute this code, we're going to make a request of the API server on that path and what do we get back? We get the actual log for that individual pod, so there we can see server listening on port 8080. And so with this authenticated kube proxy, we can retrieve any API resource on the API server. In this case, we went so far as to retrieve an individual containers log running in our hello‑world pod. Alright, so let's go ahead and foreground that kube proxy again and kill it off here and get back to our command line.

### Demo: Authentication Failures and Missing Resources

All right, so let's clear our console and start off fresh and move forward into the next demo where we're going to look at some response codes to various events, things like authentication failures, missing resources, and also what happens when we create some objects. And so let's start off with an authentication failure. The first thing that I'm going to want you to do is to create a backup of your kubeconfig file, and that's what we're doing here on line 50. I'm copying our current kubeconfig file to a file named config.ORIG to save that as a backup because what we're going to do next is make an edit to that file to break our authentication from our kubectl client to our API server. And the way that we're going to do that is by editing the kubeconfig file, and then inside of here, we're going to search for kubernetes‑admin, and we're going to jump down to the section that defines the username. So here we see, at approximately line 29, we see name: kubernetes‑admin. And what I want you to do here is to throw a 1 on the end here so that it breaks the authentication to our cluster because this username doesn't exist in our cluster. So let's go ahead and save that out. And now let's execute a command and see what happens now that are kubeconfig file is broken. And so what we're going to do here on line 66 is try to execute a command against the API server, so kubectl get pods. And what we're going to do is we're going to see what happens when this occurs and look at the response code that we get from the API server. So to see the response code, I've added the ‑v 6 parameters to increase the verbosity. Let's go ahead and run this code together. And what we'll see at the bottom here is it asks us for a username because it couldn't find that user using certificate‑based authentication because Kubernetes‑admin1 doesn't exist. And so just enter any username here. It could be anything. I'll just enter my name and a fake password. And what we'll see is we're going to get some output at the bottom here. If we scroll up past all of the output here at the bottom to the actual response from the API server, what we'll find is that the response code that we get for kubectl get pods is 403 Forbidden. We failed to authenticate to the API server, and we were not able to retrieve a listing of Pods. And so that output there indicates that we were prevented from doing that, and we failed to authenticate. And so let's go ahead and put our kubeconfig file back. We'll copy our backup over to the correct location for the kubeconfig file, and that's what we're doing here on line 69, copying config.ORIG and overwriting the broken kubeconfig file. Before we move forward, let's be sure to test out access to the API server to make sure everything is back the way we want it to be. And with kubectl get pods there, you can see we were able to retrieve a listing of Pods for the Pods that are up and running in our cluster. There we see hello‑world Pod. So we know we were able to authenticate to the cluster. And so now let's check out what happens if we try to get a resource that doesn't exist. And so I have some code here on line 75, kubectl get pods nginx‑pod ‑v 6. And so that Pod doesn't exist in the cluster, but what I want to show you here is the response code that we get back when we try to ask for a resource that doesn't exist. So let's go ahead and run that code and check out the output at the bottom, scrolling back up to the top of the output here, we can see that our kubectl get pods nginx command did execute. And we can see the GET operation returned a 404 Not Found because that Pod was not found inside of the API server. Now moving forward, let's look at what happens when we create a deployment and look at the interactions between our kubectl client and the API server. And so on line 79 here, I have some code to create a deployment‑‑kubectl apply ‑f deployment.yaml ‑v 6. Let's check out the interactions between kubectl and the API server. And so in the output at the bottom here, we see the GET operation against the default namespace for the deployments named hello‑world, and the response code is 404. So the first thing that it does when we try to create a deployment is it checks to see if the deployment exists. If it doesn't exist, then it's going to go ahead and post the deployment to the API server. And so that's the next operation that we see. We see POST in the default namespace for deployments. It's going to apply the configuration that we want to deploy, and then the response code is 201 Created. And then the standard output at the bottom, we see deployment.apps/hello‑world created. And so that interaction of testing to see that it exists and then posting it to actually create the deployment is how kubectl interacts with the API server when you try to create resources. Let's see if our resource was successfully created with kubectl get deployments. There it is, there's our hello‑world deployment that's up and running. And so for the last part of our demo, let's check out what happens when we delete resources for the API server. So on line 85 here, I have kubectl delete deployment hello‑world ‑v 6. And then in the output at the bottom, we see the DELETE operation for the default namespace against the deployment/hello‑world. We get a 200 OK because that deployment was deleted. And then we have our normal output, deployment.apps hello‑world deleted. And then just to wrap up the demo, let's go ahead and delete that Pod that we created at the beginning of the demo with kubectl delete pod hello‑world.

### Module Summary and What's Next

Here we are at the end of our module, and we covered oh so much information. We looked at the Kubernetes API and the API server, and also at working with Kubernetes objects, specifically defining objects, API groups, and API versioning. We also looked very closely with a deep dive into the anatomy of an API request, so we know exactly what's happening between our clients and the API server. Well, that's a wrap for this module. Please join me in the next module, Managing Objects with Labels, Annotations, and namespaces.

3

Managing Objects with Labels, Annotations, and Namespaces

1h 1m 26s

### Introduction, Course, and Module Overview

Hello, this is Anthony Nocentino with Centino Systems. Welcome to my course, Managing the Kubernetes API Server and Pods. This module is Managing Objects with Labels, Annotations, and Namespaces. So let's take a look at where we are in the course so far. We just wrapped up our deep dive into the Kubernetes API with the module Using the Kubernetes API. Now let's learn how we can use some techniques such as labels, annotations, and namespaces to organize resources and objects in our Kubernetes cluster in the module Managing Objects with Labels, Annotations, and Namespaces. In this module, we're going to discuss organizing objects in Kubernetes, and the techniques that we're going to use to organize objects are going to be namespaces, labels, and annotations. Once we have those principles behind us, we're going to learn how Kubernetes uses labels to manage critical system functions such as managing services, controlling deployments, and workload scheduling in our cluster.

### Organizing Objects in Kubernetes

Now in this module, we're going to learn how to organize objects in Kubernetes. And the three primary methods with which we can organize objects in Kubernetes are going to be namespaces, labels, and annotations. Now we will learn throughout the module the appropriate place to use these different types of organizational methods. But from a high level standpoint, let's go ahead and touch each one of these right now. You'll want to use namespaces when you want to put a boundary around a resource or an object in terms of security, naming, or resource allocation. You'll want to use labels when you want to act on an object or groups of objects or influence internal Kubernetes operations. And then, finally, annotations are when you want to add just a little bit more additional information or metadata even about a particular object or resource.

### Introducing and Working with Namespaces

Now, for the first organizational method in Kubernetes that we're going to look at, we're going to check out namespaces. What namespaces give you the ability to do is to subdivide a cluster and its resources, conceptually giving you a virtual cluster or virtual clusters to deploy objects into. You deploy objects into a namespace. The primary reasons why you want to use namespaces in your cluster is to give you some resource isolation or organizational boundaries inside the cluster. Now, some common grouping concepts behind using namespaces is multitenant clusters. Maybe you want to group on application environments such as prod, QA, and test or different application stacks that you're deploying into your cluster. Maybe you want to group on users or application owners or different teams, and things like that. It comes down to what are the business requirements for your organization, and how do you want to subdivide your cluster leveraging namespaces? Perhaps mixing environments in your cluster like prod, QA, and test isn't a good idea and you want to use separate clusters for that, but the opposing force to that is for a business standpoint, well, maybe you need to leverage all of the resources in your cluster and you do need to mix environments within your cluster. So those are some of the design decisions that you'll have to go through and look at the business requirements you're trying to achieve leveraging namespaces. Now, from a technical standpoint, we can leverage namespaces to assert some resource control upon namespaces, limiting to things like CPU, disk, RAM, number of Pods, and so on. So let's continue looking at namespaces and the benefits that they can provide for us in our Kubernetes cluster. First up is it's a security boundary for role‑based access control. We can limit who can access what inside of our cluster based on namespaces. We can leverage both role‑based access control and namespaces in different combinations that get users access or prevent access to resources in the cluster. We also can leverage namespaces as a naming boundary. I can have a resource in two separate namespaces have the same name, because a namespace is a boundary around naming. I do want to point out that a resource can only be in one namespace at a point in time. And then finally, I do want to drill this point home before we move forward, that Kubernetes namespaces is an organizational construct inside of our cluster. It has nothing to do with the concept of the Linux operating system namespace. Working with namespaces. When you want to work with namespaces, much like any other resource inside of Kubernetes, you have the ability to create, query, and delete a namespace. Leveraging namespaces, we can also perform operations on objects inside of a namespace. Perhaps I need to delete all of the Pods in the namespace collectively, and I can do that. And I can also delete the entire namespace and all of the resources within that namespace. Now, some objects in Kubernetes are namespaced, meaning they can be placed into a namespace, and some aren't. So let's look at some that are and some that aren't. Some of the resources that can be namespaced are Pods and Controllers and Services, right, workload that gets deployed into the cluster. And I like to think the things that aren't namespaced are, generally speaking, the physical things, things that we can touch, things like PersistentVolumes and Nodes. Now, conceptually, let's go ahead and look and see what a namespace looks like in our cluster. So in our cluster, let's say we went ahead and declared two namespaces, NS1 and NS2, and so these are two organizational units inside of our cluster, we have two namespaces for us to deploy resources into. And so let's go ahead and start that process. We're going to deploy some Pods. So here you can see we have two Pods with the same name, Pod1, deployed into each of the namespaces in our cluster. They can have the same name, because a namespace is a naming boundary. We can kind of continue that concept here, you can see Pod2 coming along. Again, normally that would be a naming conflict within a namespace, but since they're in two separate namespaces, we have that there. Now this kind of design pattern could be used to deploy for down‑level environments, multitenant clusters, and things like that. We can go and add other resource boundaries or security boundaries to these namespaces for administrative control over who can do what in our cluster. Now, continuing to look at namespaces, let's talk about some of the namespaces that are available by default for free when you create a Kubernetes cluster, and also how we can create our own namespaces. Now, some of the namespaces that we get out of the box in a Kubernetes cluster are the default namespace. The default namespace exists for when you deploy resources into the cluster and don't specify a namespace. This is the default namespace. There's the kube‑public namespace. This namespace is created automatically, and is readable by all users in the cluster. This is commonly used to store shared objects between namespaces for access across the whole cluster, so commonly used to store things like config maps. There's kube‑system. Inside of kube‑system, you'll find the system Pods, so things like the API server, etcd, the Controller Manager, and kube‑proxy, and so on. Now, in addition to these namespaces here, we have the ability to create user‑defined namespaces for us to be able to deploy a workload or resources into, and we can create a user‑defined namespace in one of two ways. We can do it imperatively at the command line with kubectl or declaratively in a manifest in YAML or JSON.

### Creating Namespaces and Creating Objects in Namespaces

So let's discuss how we can create namespaces and also create objects in namespaces. And we're going to start off with creating the namespace declaratively in YAML. And so just like any other manifest, we start off with our API version, and it would declare the kind, in this case, a namespace. To give our namespace a name, we'll go under metadata, and we see name. In this case, the name of our namespace is going to be playgroundinyaml. Now to declare an object in a namespace, let's go ahead and start that process. So here you can see my API version is going to be apps/v1. The kind is going to be a deployment. And for me to deploy this deployment into a namespace, I specify the metadata and declare the namespace as the existing namespace already, in this case, playgroundinyaml. To do this imperatively at the command line, I can use kubectl create namespace and then just give my namespace the name, so in this case, playground1, creating a totally separate namespace in our cluster when compared with the example up top here. If I want to declare an object in the namespace, I can use this syntax. I say kubectl run. The resource in this case is going to be nginx. I'm going to feed it an image named nginx. And at the end there, you can see ‑‑namespace, and then the name of the namespace I want to deploy this object into‑‑playground1.

### Demo: Working with Namespaces and Objects in Namespaces

Time for our first demo in this module, and we're going to do a lot of work. We're going to create a namespace, add some resources to a namespace, query a namespace, interact with some of the resources in a namespace, and then finally delete all of the resources in a namespace by deleting a namespace. So here we are in VS Code. Let's go ahead and get started with our first demo, and at the bottom here, you can see I have a connection open to c1‑master1. The first thing that we're going to do in our Kubernetes cluster in this namespace demonstration is get a listing of all of the namespaces that are available in our cluster, and to do that, we can use kubectl get namespaces, and that's going to list the three currently‑provisioned namespaces in our cluster. We have default, kube‑public, and kube‑system. Now I'd like to show you how to ask Kubernetes what resources can be in a namespace and what resources can't be in a namespace. And we've used this command so far in this course, kubectl api‑resources, and that lists all of the API resources that our API server knows about. And in this case, I'm going to add an additional flag here that says namespaced=true. What that's going to do is give me a list of all of the API resources or objects that can be part of a namespace. And so let's go ahead and run this code here, and I'll pipe that in ahead just to limit some of the output, just to get a sample of what is a namespace of an object that's available on our Kubernetes cluster. And so here you can see things like pods, and podtemplates, and endpoints, and things like that. On the right here, we can see a NAMESPACED column and the attributes for all of those is set to true. And so these are the resources that we can provision or deploy into a namespace. Now let's flip the coin a bit and go ahead and set that the false. So we're going to ask Kubernetes, kubectl api‑resources ‑‑namespaced=false, pipe that in ahead. And now let's get a listing of some of the objects that can't be part of a namespace. So in the output here, we can see various resources such as namespaces, nodes, and persistentvolumes. These are all resources that can't be part of a namespace. Now I do want to go ahead and pull some additional information about our namespaces, and we're going to use kubectl describe namespaces to give us that, and that's going to give us some deeper‑dive information about all of the namespaces that are up and running on our cluster. And let's walk through some of this output on the bottom here. So here we see kube‑system, right, the name of our namespace. There's no labels or annotations associated with it, and we can see its Status is Active. Namespaces actually do have status. When they're active, they're up and running, and provision, it could be in the state where you're deleting the namespace, and it's terminated and shutting down the resources in the namespace to be deleted, and then deleting the namespace. Now we also have some additional information at the bottom here. We see No resource quota and No resource limits, right? So there's no resource constraints around this particular namespace. If we scroll up in our output, we see the kube‑public namespace with very similar output, and also the default namespace. So let's say we wanted to pull that detailed information for an individual or named namespace. Here, we can do the same thing. We say kubectl describe namespaces, and just at the end there we specify the individual namespace that we want to operate on, holding back that describe data for that particular namespace, in this case, kube‑system. Now let's say we wanted to get a listing of all of the pods across all of the namespaces in our system, so regardless of where they are in what namespace they're provisioned into it. And so here we can do that with kubectl get pods ‑‑all‑namespaces. Let's go ahead and run this command here, and we can see all of the pods across all of the namespaces in our cluster. In this case, I only have pods running in kube‑system because we have no workload deployed into our cluster yet. Now that was just for pods. What if I wanted to ask Kubernetes to give me all of the resources across all of the namespaces? And so let's go and see how we can do that. We can say kubectl get all ‑‑all‑namespaces, and that's going to give me a listing of everything that's deployed no matter what resource type it is across all namespaces, so a useful debugging command when you have to get a big‑picture view of the world. And at the bottom here, we can see some of the output from the resources across all namespaces. You have replicasets, and deployments, and daemonsets, and services, and things like that, all of the system‑level functions that we've deployed when we built this cluster together in our previous course. So let's go ahead and keep moving along, and let's go ahead and learn how we can query a particular namespace or a named namespace. So far, we've been working with getting resources across all of the namespaces, and if I want to get the pods just for an individual namespace in our cluster, I specify it like this, I say kubectl get pods ‑‑namespace kube‑system where that kube‑system is the namespace I want to query. And again, we get our list of all of the pods in that individual namespace. Now let's go ahead and move forward and learn how we can create some namespaces together, and we're going to start off with doing this imperatively, or at the command line. I'm going to say kubectl create namespace, in this case I'm going to say playground1. Let's go ahead and run that code there, and you can see at the bottom we get output that says namespace/playground1 is created. Now I want to go ahead and take another shot at creating a namespace named playground1 as well, but notice that there's a capital P there. let's go and see what happens here. We get an error, The Namespace Playground1, with a capital P, is invalid. It fails a regular expression match for the naming convention for namespaces, it needs to be all lower case, alphanumeric, things like that. And so we get some good detail about what's expected in the error there. So in this case, it's not that it's a duplicate name, but it's going to throw an exception that it can't create a name with an uppercase letter. So let's go ahead and declaratively create a namespace now. I'm going to go ahead and look inside of this file here that I have. This will be available in the downloads for you. At the bottom here, inside of namespace.yaml, we can see the API version for the resource that we want to create is v1, kind is Namespace, metadata, and then the name of the namespace is going to be playgroundinyaml. And just like any other YAML file, let's go ahead and deploy that into our cluster, and here we say kubectl apply ‑f namespace.yaml, run that there, and we can see that our namespace, playgroundinyaml, was created. So if we go ahead and get a listing of all of the namespaces again with kubectl get namespaces, here you can see we have those two additional namespaces that we just created, playground1 and playgroundinyaml. Now let's go ahead and deploy a deployment into our playground1 namespace, and I have a deployment.yaml up here, let's go ahead and take a peek inside of that, and here we can go through kind of at the top‑level output here, we have the API version for the deployment that we want to deploy as apps/v1, we see the kind is Deployment, and in the metadata section here, we specify the name, and any labels associated with its resource, and then the namespace as an attribute of metadata, and we're going to assign it to the playground1 namespace. And so let's go ahead and jump back over to our demo script here, and send that deployment into our Kubernetes cluster, and we're going to do that with kubectl apply ‑f deployment.yaml. That's going to create our deployment hello‑world in the playground1 namespace. Now I want to do one more thing here to show you how to do that imperatively at the command line. So here, I specify the fact that I want to create a hello‑world pod. And so let's walk through this syntax here. So here we see kubectl run hello‑world‑pod, and we're going to specify the image as our hello‑world‑app that we've used several times so far, and we're going to use a pod generator just to create an individual pod. Now that last line there is the most important , ‑‑namespace playground1. So this resource is going to be at the command line, we're imperatively creating this resource and specifying the namespace that we want it to go into. So let's go ahead and use kubectl get pods to see where our workload is, and, well, there's no pods. Where are the pods? Well, we deployed them into a namespace, and when we use kubectl get pods, well, we're asking for the pods from the default namespace. And so let's go ahead and specify the namespace to which those pods live in. And to do that, we can use kubectl get pods ‑‑namespace, and then the namespace that we deployed all of that workload into, playground1. So we should see the four that were associated with the deployment that we created and the one individual pod that we created at the command line imperatively. Now if you get tired of typing ‑‑namespace, you can shorten it up to ‑n, and we'll get the same output here. So we just say kubectl get pods ‑n playground1, and get the same output if you get a little tired of typing ‑‑namespace all the time. Now if we want to work with a particular namespace and kind of get a feel for everything that's running inside of that namespace, well, earlier we specified kubectl get all across all namespaces, and we saw everything. Well, we can use that same concept against a particular namespace, and we can say kubectl get all in a named namespace, in this case playground1, let's go ahead and run that code there, and we can see the various resources in this particular namespace. So we have the five pods that we deployed, we have our deployment, and then we have the replica set that supports that particular deployment.

### Demo: Performing Operations on Objects in Namespaces

Now let's go ahead and move forward and see how we can perform some operations on resources in a namespace. And so at line 61 here, I'm specifying kubectl delete Pods all in the namespace playground1. So this is going to attempt to delete all of the Pods in this particular resource. And so let's go and do that together and see what we get. So there we see the four Pods associated with a ReplicaSet and that one Pod that we deployed imperatively are being deleted. Now let's go and check the status of our deletion with kubectl get pods. I'm going to specify ‑n for the shortened version of the namespace parameter and then the namespace playground1. Let's go ahead and execute, oh hey, what happened here? I thought I was supposed to delete all of the Pods across the namespace. Well, I did. I deleted all the Pods. In fact, all the Pods got deleted, and then they were recreated by the ReplicaSet that supports that deployment. We deleted the Pods but not the deployment. The individual Hello World Pod that we created imperatively, well, that's gone. But Kubernetes did what it's supposed to do. It deployed four new Pods after we deleted, the Pods that were part of that ReplicaSet, which was part of our deployment. So if we want to take it all out, if you want to get rid of all the resources in a particular namespace, let's look at how we can do that because, in this case, I want to get rid of the Pods, I want to get rid of the deployment and the supporting ReplicaSet. So let's go ahead and specify kubectl delete namespaces playground1. Now what's going to happen is it's going to delay the deployment, the ReplicaSet, and all the Pods associated with it behind the scenes inside of that namespace. In fact, it's going to delete anything that is inside of that namespace. All right, so with that namespace deleted, let's go and delete the other namespace that we created together just to clean things up. And if I go ahead and ask Kubernetes to give me all the resources in the default namespace, I don't see anything there. And if I ask Kubernetes to get everything across all namespaces, this should just be the system level stuff. Let's go ahead and scan through it. Here we see all of the Pods as part of our kube‑system. We see the services, DaemonSets, deployments, and ReplicaSets associated with our cluster function. So all of that user workload that we created together is gone.

### Introducing and Working with Labels and How Kubernetes Uses Labels

Now the second method that we're going to use to organize workload or objects in our Kubernetes cluster is labels. Labels are used to organize resources, such as pods or nodes, or honestly, any resource in Kubernetes. With labels we can leverage label selectors to select or query those objects, and what that's going to do is return a collection of objects that satisfy the search conditions that we provide in those label selectors. What this enables us to do is perform operations on those collections of resources, like pods. Perhaps I want to query a subset of pods and perform some operation on all those pods, like delete, or some other administrative operation. Now it's not just for us to perform operations on workload; labels are also used to influence the internal operations of Kubernetes, and we're going to look much more closely at that later on in this module. So exactly what is a label? Well, a label is a non‑hierarchical key/value pair. We assign a key, we give it a value, and that is then associated with a particular resource in our cluster. We can query on the key and retrieve that value. Now, objects in Kubernetes can have more than one label per resource. And what that gives us the ability to do is that enables us to build more complex representation of the state of our system and query on those attributes or query on those labels associated with the objects in our cluster. From a syntactical standpoint, keys can be 63 characters or less, and values have to be 253 characters or less. If we need to exceed that length in terms of value or data that we need to store in a label, there's another method in which we can do that, and we're going to look at that later on in this module. Now, let's discuss some of the operations that we can perform when using labels. Well, first off is we can create resources with labels already assigned, and we can do this both imperatively at the command line with kubectl or declaratively in a Manifest with YAML or JSON. That gives us the ability to create resources with labels already assigned. If we have something already deployed in our cluster, we can edit an existing resources' labels, and we can assign new labels or overwrite existing labels, if we need to. Now, let's look at what it takes to add and edit labels in resources in Kubernetes. And first off, let's look at how we do this declaratively in code in YAML. And so as we start off with any other YAML definition, we start off with the API version. In this case, we're going to work with a pod. Now, in the metadata section, we've gone ahead and used the name parameter many, many times before. In this case, name is going to be nginx‑pod. Here's where we'll go ahead and assign our labels. We specify labels, and then the key and the value for the label that we want to specify. In this case, the key being app, and the value being v1. We can specify multiple labels in the section as well. We can say tier equals PROD, assigning the tier key with the value POD. And then we go about filling out the rest of the Manifest, in this case, the spec section of our pod that we want to deploy. Now, that's declaratively in code. Let's look at how we do this imperatively at the command line with kubectl. We say kubectl label, then a resource type, in this case pod, then a resource name, nginx, and then specify the keys and the value, tier=PROD, app=v1. If we need to change a label, let's go ahead and look at the syntax for that. From left to right here, we see kubectl label pod nginx. We're going to change tier from PROD to tier to DEBUG, so we see tier=DEBUG, and then again app=v1. Now, we also need to add the ‑‑overwrite parameter to overwrite that label that already exists, changing it from PROD to DEBUG. Now, to delete a label, let's look at the syntax for that. We say kubectl label pod nginx, and then the label or the key that we want to remove, in this case, app followed by a minus sign, and that will remove that label, that individual label app from this particular resource. So we've talked so far a lot about the semantics of using labels in Kubernetes, but let's get a graphical representation as to why we would use labels in our Kubernetes cluster. So let's say we have a cluster up and running, and we've deployed some workload in it. Now, what if I needed to work with a subset of these pods or these workload in this particular cluster at a point in time? Well, I can leverage labels and assign labels to particular types of workload. Perhaps these three pods with this blue label here are part of one application, and these three pods over here with this magenta label are part of another application. I can use labels to write queries and select these subsets of pods to perform operations on them. So in Kubernetes, we can query using labels, and we're going to find some selectors to help us execute these queries. And so let's walk through some examples at the command line here. So we can use kubectl, and say kubectl get pods, and we've done this many, many times where that gives us a listing of all the pods. Now, if we specify ‑‑show labels, that's going to list all the labels associated with those pods, and honestly, this could be any resource. That could be nodes or services, whatever it is. If we wanted to find a selector and perform a query, here's how we do that. So kubectl get pods again, ‑‑selector, followed by the query that we want to execute. The selector being tier=prod. So this is the label where the key is tier and the value is prod. So, any pods that match this selector where the key is tier and the value is prod will come in the output list of kubectl get pods. Let's go a little bit further and look at another example. Kubectl get pods. If I get tired of typing ‑‑selector, I can switch to ‑l, and I can perform a more complex query. So here you see 'tier in (prod, qa)'. And so anywhere my label has a key of tier and a value of prod or qa will come in the output list of this kubectl get pods command because of what we defined in that selector and the labels that satisfy that query. Another example being kubectl get pods, and say ‑l. I can say 'tierr notin (prod, qa)'. That's going to get me any label that doesn't satisfy that query. And then, finally, this doesn't have to be just for pods, it can be for any resource in Kubernetes. So I can say kubectl get, whatever resource I want it to be, in this case, nodes, and specify the labels, either ‑‑show‑labels, or define a selector and build a query to get me that subset of those resources that I want to work with. So let's look at how Kubernetes uses labels. So far, we've looked at how we can use labels to get collections of objects or resources from Kubernetes at the command line using labels and selectors, but let's look at how Kubernetes uses this internally for some of the functionality that it provides to us. First up is Controllers and Services match Pods using selectors. This is how Kubernetes knows if this subset of pods belongs to this deployment and that ReplicaSet, or that's how Kubernetes knows that these pods are part of a particular service. They match on the label selectors so that they can figure out who belongs to which resource. In addition to controllers and services, Kubernetes also uses labels to influence the scheduling of pods onto specific nodes in our clusters. We see this commonly when we have special hardware where we have local SSDs or GPUs, and I want to say, hey Kubernetes, I need you to run this pod on this type of hardware in my cluster. And you don't really want to get in the business of pinning a particular pod to a particular node name because, well, that node name might change. But you can leverage labels to assign a label to a subset of nodes, or a particular node, but you can then in the future move that label to another resource or another node in your cluster, and so we get away from pinning it to particular nodes. And how this will all work? Behind the scenes, it uses label selectors. Now, in the next upcoming slides, we're going to go through each one of these scenarios specifically to get a visualization of how this all works. So let's go ahead and start that now.

### Using Labels for Services, Deployments, and Scheduling

So let's see how Kubernetes uses labels and selectors to find out which pods are members of services in its cluster. So let's say we have a cluster up and running and we deploy a collection of pods and we front end that with a service, in this case, an HTTP service. We're going to go ahead and expose that to our clients, but how does Kubernetes know if the pods on the left here in our cluster are members of this HTTP service? Well, it does this with labels and selectors. In the definition of our service, we specify a selector that is used to query the pods to determine who are the valid endpoints for this particular service. And so in this case, let's say we have a selector, like A, and in our pods, well these pods have labels that match. So these pods will satisfy the selector for that particular service. All those endpoints for these individual pods will be registered as part of that service and that traffic will be distributed to them. Now let's say we come along and administratively we remove the label from one of these pods. What's going to happen is this individual pod is going to be deregistered as an endpoint from this service and workload is no longer going to be distributed to it. So we just look at services and how they use labels and selectors to determine who is a member of a service, let's look at controller operations, specifically deployments, and how we can use labels and selectors to determine which pods are part of the replica sets associated with deployments. So in a cluster here let's go ahead and say we instantiate a deployment and our deployment kicks off a replica set with the label R1. That's going to instantiate a replica set and create pods associated with that replica set. Now, each one of these resources will have a label and a selector for the lower level resource, so the deployment will have a selector that queries the replica set that makes that part of its active deployment, and then the replica set will have a selector associated with it that will then select the pods that are a member of that particular replica set. Now, let's say administratively we come along and we want to move between this first version of our application and a new version of our application. What happens here, is, the deployment switches and kicks off the creation of that second replica set in our cluster, and then based on our rollout strategy, pods will start moving between the two replica sets with the newer version of our application. So these pods here are created part of the second replica set, which is part of the current replica set in our deployment. Now, if we come along and we remove a label for a pod that's a member of this replica set, well, that pod will no longer satisfy the selector for that replica set and so it will be removed from the replica set. Now, our replica set will possibly recover and create a new pod if it's configured to do so. So let's look at the final way in which we can influence the internal operations of Kubernetes with scheduling pods to specific nodes. Now, let's say we have a couple nodes in our cluster, and in this case, this cluster has some hardware, which, some nodes have SSD and some notes have GPU and some nodes have both. And so let's go ahead and assign some labels to these nodes that represent that state, and so let's say the node on the left here has both SSD and GPU and the node on the right here has just SSD. When we define our pods, we can declare in the pod spec a node selector, and in that node selector, we can specify a query for a key and a value that we want this pod to be scheduled to. And so pods that need SSD can be assigned to nodes that have SSD, pods that need GPU can be assigned to nodes that have GPU, and so as our workload gets scheduled, you can see here that the node on the right gets just SSD and the node on the left there gets pods with labels that match on both SSD or GPU. Now let's walk through the YAML code needed to define deployments and services in terms of how do we know which pods are part of a deployment or a service, and so, when specifying a deployment in YAML, we start off with declaring the kind is deployment, then skip a few lines and jump down to the spec for this particular deployment. In the spec, we're going to define a selector. In the selector, we're going to define matchLabels where we're going to define the label or the key and the value for the pods that are going to be members of this deployment, or under the control of this deployment. Now, further along in our deployment, we define a pod template. In a pod template, we define metadata where we specify the labels for the pods that are created by this template, right? These are the ones that are going to get stamped out by the replica sets that support this deployment. Here we specify the labels as the same as the matchLabel at the top here. We say run: hello‑world. So this is the same label or key and the value that we need to match and the selector at the top here. So these are critical that they match. These pods that are created with this template will satisfy the selector and be part of this deployment. We go along and fill out the remainder of our deployment and push that code into the API server, and everything will be created as we need. Now from a service standpoint, when we specify the kind equals service and fill out the remainder of the definition for a service, in the spec for a service, we also define a selector. In that case again, we'll define the key and a value for the label that we want to satisfy as part of this service. So we say run equals hello‑world. These two need to match, right? The pods that are created by our deployment, if we're going to front end those by a service, the labels in the pod template, as those pods are created as part of the replica set, will need to match or satisfy the selector of the service on the right here, and then once we have that, we go ahead and fill out the remainder of the data that's required for our deployment.

### Demo: Working with Labels - Creating, Querying, and Editing

All right, so here we are in a demo, starting it and off with working with labels. Then we'll learn how to interact with Pods by both name and label. And with that behind us, we'll learn how we can influence Kubernetes resource management with labels and selectors, specifically in services, deployments, and Pod scheduling. Here we are in VS Code with a session open to c1‑master1 at the bottom here, and we're going to start off our demonstration with working with labels by creating a collection of Pods with labels assigned to each Pod. And I have a YAML file here that I want to review with you so that you can go through and see how we created these individual Pods with the labels. And here you can see at the very top our apiVersion is v1, kind is Pod, and then in the metadata section we see the name of our Pod, in this case nginx‑pod‑1, and there we define a collection of labels. In this case, I have three labels, the keys and then the values specified, so we have app equals MyWebApp, deployment equals v1, and tier=prod. And so those three labels will be assigned to this resource. We have the rest of the Pod spec at the bottom here defining the container, the image, the ports, and so on. Here we have another definition for nginx‑pod‑2. Again, we see the apiVersion, the kind, and in the metadata section, the information about the labels that we want to assign to this Pod here. So here we see app equals MyWebApp, deployment equals v1.1, and tier equals prod, and then again the Pod spec at the bottom. Let's look at one more. I have nginx‑pod‑3. In this case, the labels are app equals MyWebApp, deployment equals v1.1, and tier equals qa, the rest of the Pod spec. And I have another Pod at the bottom here that, if you want to look more closely at you can on your own, but there is nginx‑pod‑4. So with that, let's go ahead and jump over to our demonstration again and create these Pods in our cluster. Let me go ahead and feed that configuration into our API server with kubectl apply ‑f, feed in our YAML file, and in the output at the bottom there we can see nginx‑pod‑1 through 4 successfully created. Now the first thing that we're going to do when working with resources with labels, or, in this case, we're going to be working with Pods, I'm going to ask kubectl to get me a list of the Pods, but I'm going to add the parameter ‑‑show‑labels, and that's going to give us this output here. We get our standard kubectl get pods output with regards to the name, if it's ready, the status, restarts, and age, but we get an additional column of data that gives us all of the labels for these Pods. And so there's all the various labels that we specified in our Pods definition in the YAML file. Now, in addition to kubectl get pods, we can also look at an individual Pod's labels by using kubectl describe. So at line 13 here we can see kubectl describe Pod nginx‑pod‑1, and I'm going to go ahead and pipe that into head just to limit some of the output. And here in the output for this particular resource we see the three labels in our output that we defined in our YAML file. So let's go ahead and move forward in our code, and we're going to work with querying labels using selectors. And so let's go ahead and start that process. On line 16 here we see we have kubectl get pods ‑‑selector, and then I'm specifying a single label, tier=prod. Let's go ahead and see what that gets us in the output here. And we can see in the output 1, 2, and 4 are the Pods that match on that label selector. If we go ahead and flip it and do kubectl get pods, specifying ‑‑selector tier=qa, we should get, there you go, Pod number 3, so that's our set of Pods. Three are in prod; one is in QA. If we get tired of typing ‑‑selector all the time, we can shorten it up with ‑l. Let's go ahead and run that same query as we did before, and there you can see for the tier=prod selector, we get 1, 2, and 4 back. Now, in addition to running our queries with selectors, we can also ask Kubernetes, kubectl, to also show the labels as well. So in this case, what we're doing here saying kubectl get pods ‑l, specifying the label selector as tier=prod, but also asking to output all of the labels associated with the resources being displayed by kubectl get pods. So we get a visual on the additional labels associated with the Pods that match on the selectors specified. Now, let's go ahead and look at how we can use a selector to look for multiple labels and build more complicated queries using a selector in kubectl get pods. And so at line 22 here, we have kubectl get pods ‑l. And in this case, I'm asking to match on two labels, tier=prod and app=MyWebApp, and I'm also adding ‑‑show‑labels so I can see the labels in this output. And there we can see it limits our set to Pods 1 and 2. So these are our Pods that match on app=MyWebApp and also match on tier=prod. So let's go ahead and move forward and look at another query that we can execute. In this case, we're going to add a not equals to the selector that we're working with. And so here we can see kubectl get pods ‑l tier=prod, app!=MyWebApp. And I'm also adding the ‑‑show‑labels flag at the end there. And there we see we just get nginx‑pod‑4 because that's going to match on not MyWebApp. In that case, if we look at the label output at the bottom, we see app=MyAdminApp, and this Pod is output because it's going to satisfy that query by not matching on the app!=MyWebApp but matching on tier=prod. We can also specify an in clause in our query. So here we see kubectl get pods ‑l tier in (prod,qa). And what this is going to hit on, or match on, is where tier is prod or qa, and both of those will be satisfied in this query. And so here we see all of the Pods are either in prod or qa. And so if I invert that query, I can say tier notin prod or qa, what we should get here is an empty set, and we do, because no resources matched on that particular query. Now let's look at how we can output a particular label in column format when working with kubectl get pods. Now look at the code here on line 28. We have kubectl get pods ‑l, but capital L, so ‑L, and then I specify a key that I want. Let's go ahead and run that code and see what the output gives us. So we still get the standard kubectl get pods output, but we added an additional column there with the key equals TIER and the value for each of the individual labels, so prod, prod, qa, and then prod for all of the Pods that are currently running in our cluster. We can combine that with additional labels. In this case I'm going to tier and app, and that's going to add an additional column output to the output of kubectl get pods, so there we have a column for tier and a column for app, specifying all of the values associated with those resources in the kubectl get pods output. So let's look at how we can work with labels and edit an existing label and adding a new label. And so first up, let's go ahead and edit an existing label. So on line 32 here we see kubectl label pod nginx‑pod‑1. So I'm going to change a label for an existing label on this individual Pod. And there we see tier=non‑prod and I added the ‑‑overwrite parameter. And so let's go ahead and execute that code there, and that's going to change the value for the key tier for this individual Pod for that label. I run that code there, and at the bottom we can see nginx‑pod‑1 is successfully labeled. If we do a show labels for that individual Pod, we'll now see that that label is updated from tier=prod to tier=non‑prod, just for nginx‑pod‑1. Now if I want to add an additional label to a resource, on line 36 we see kubectl label pod nginx‑pod‑1, and I specified the additional key and value. So in this case, the key is another and the value is Label. Go ahead and run that code, and we can see that we successfully labeled our pod‑1 at the bottom here with the output. If we ask kubectl get pods to show labels again for that individual Pod, on the right we can see in the LABELS section, we have another=Label as part of that output set there, in addition to all of the existing labels. If we want to get rid of the label that we just added, we go ahead and specify kubectl label pod nginx‑pod‑1, and then we specify the label or the key, another, followed by a minus sign. Let's execute that, and there we can see that it says pod‑1 labeled, which is a little weird. Maybe it should say something like pod‑1 label removed, but it successfully did the work that we asked. Because if we do show labels again, we can see we're back just to the three labels that we started with for app deployment and tier. So far, we've looked at adding a new label and removing an existing label on individual resources. What if we wanted to perform an action on a collection of Pods? And so let's look at what we're going to do next here on line 44. We're going to execute a command, kubectl label pod ‑‑all, and we're going to update the label with the key tier, and we're going to set that to non‑prod, adding the overwrite parameter. And what that's going to do is update all of the Pods in this namespace, or in our default namespace, since we're not specifying a namespace, setting the label to tier=non‑prod. And so let's go ahead and execute that code there. And we can see pod‑1 wasn't labeled, and pods 2, 3, and 4 were. Pods 2, 3, and 4 were set the prod. They were then updated to non‑prod, but we had previously updated pod‑1 to non‑prod earlier so we can see that's that specify as not labeled because there wasn't a change event on that individual resource. So if we go ahead and get a listing of all of our Pods and their subsequent labels, here we can see on the right everyone's been updated to tier=non‑prod. Now, moving forward in our demo, let's look at how we can perform an operation on a subset of Pods with a selector. And so here we're going to use kubectl delete pod, and we're going to specify ‑l to specify a selector, and the selector is going to be tier=prod. So anything that has the key tier and the value non‑prod in the label is going to be deleted when I execute this code. So let's going ahead and do that there, and we can see Pods 1, 2, 3, and 4 are now deleted. And if we get a listing of our Pods with kubectl get pods, we can see that there are no resources found in this default namespace that we're working with.

### Demo: Deployments, ReplicaSets, Labels, and Selectors

So we just went through some basic label operations, adding, editing, and then performing operations on collections of resources with label selectors. Now let's shift gears and look at Kubernetes resource management and how Kubernetes uses labels to make decisions on resources in the cluster. And so let's go ahead and bring some new code in the view here, and the first thing we're going to do is create a deployment. So I'm going to go ahead and open up this code right here for a deployment‑label.yaml, and let's walk through the code on the screen here. So, we're declaring a standard deployment. We've done this many times in this course and previous courses. And let's go ahead and look at the spec for this deployment. Let's bring that into scope here. Now, in the spec, we see the selector on line 9 defining matchLabels on 10 where the label has a key of app and a value of hello‑world. Now the pod template for this individual deployment also specifies some metadata for the pods that will be created by this deployment, and the metadata for each of those pods is going to have a label of app=hello‑world. Now, let's go ahead and jump back over to our script here and deploy that deployment into our cluster. So there we see hello‑world created. Now we also have a service that we're going to associate with this deployment. So let's jump over and view this code in service.yaml. In this code, we see at line 10 in the spec for the service, we see a selector with the key of app and a value of hello‑world and that label being defined. And so that's going to be used to specify which pods are the endpoint of this service. And so let's jump back over to our script again and deploy that into our cluster with kubectl apply. And there our hello‑world service is created. So let's take a look at how labels and selectors are used in deployments, ReplicaSets, and pods to see how it all pieces together. And so the first thing that we're going to do is ask Kubernetes to give us some detailed information about our deployment, and we're going to do that with kubectl describe deployment hello‑world. So let's go ahead and run that code. And it kind of runs off the screen a little bit. We're going to scroll up in this output and look at some of the data in here. So here we see our selectors for this particular deployment are app=hello‑world. And if you go a little bit further in this detailed output, we see the pod template. There we see any pod that's going to be created by the ReplicaSet behind this deployment will have the label as app=hello‑world. That deployment created a ReplicaSet, and so let's look more closely at that ReplicaSet. So, kubectl describe replicaset hello‑world. And so let's look at the detailed information for the individual ReplicaSet for this deployment. So if we bring that into view here, we can see the name of this ReplicaSet is hello‑world‑5646fcc96b. Well, that's interesting. Where'd that value come from? Well, that's part of what's called the pod‑template‑hash, which is going to be part of the selector that helps define what are the pods that are members of this particular ReplicaSet. And so let's look at the selectors associated with this ReplicaSet. So we see app=hello‑world, and pod‑template‑hash, and the value there is 5646fcc96b. Now, there's also some labels associated with this resource, and there we see app=hello‑world, and also the pod‑template‑hash again. Now, the selector here with app and pod‑template‑hash is how Kubernetes, or how the ReplicaSet specifically, is going to know which pods match the selectors when it goes and looks at the set of pods that are running in our cluster. It's going to ask with a selector to see who matches on these keys and values. Right? App=hello‑world, and pod‑template‑hash= 564, so on. If we skew away from that and we get an answer from the cluster that says we're outside of the desired number of replicas, then our ReplicaSet is going to react accordingly. If we go down a little bit further in this output here, we see again the pod template specifying any of the pods that are created from this ReplicaSet are going to get these labels where app=hello‑world with the pod‑template‑hash specific to this ReplicaSet. So, let's go ahead and break out of this output here and look more closely at the pods that are up and running in our cluster, and we're going to ask Kubernetes to give us a listing of all the pods and showing the labels with the ‑‑show‑labels parameter. So, kubectl get pods ‑‑show‑labels, execute that there, and we can see that our four pods that we requested in our deployment, which kicked off the ReplicaSet, which then created all the pods, are up and running. And on the right there in the labels, we can see the app label and the pod‑template‑hash label, app=hello‑world, and pod‑template‑hash=564, so on. So we know we're in the desired state here. We have our four pods up and running because that's what we requested. Now let's go ahead and edit the pod‑template‑hash label for one of these pods and see how Kubernetes reacts. And so let me go ahead and bring that code into view here. And I'm going to grab a pod name, put it in my clipboard, and paste it right here in the code. Just grab pod off the top of the list there. And so let's walk through an update, the pod‑template‑hash, from the correct value of 5646fcc96b, and just set it to the value of debug. And so let's walk through that code here together on line 72. Kubectl label pod, and we're going to update this individual pod that we just copied and pasted the name into, and there we see we're adding pod‑template‑hash=DEBUG with the overwrite parameter. And so I'm going to go ahead and update that label now with this code. It's successfully labeled with the output indicated at the bottom. So now let's go ahead and ask Kubernetes for a listing of the pods with kubectl get pods, and also specify ‑‑show‑labels and see what it did. Well, here we can see we have a fifth pod up and running, right? Kubernetes reacted. It saw that when it ran the selector, it didn't find four pods, it found three pods because we updated one of those pods by changing the pod‑template‑hash. Right? The ReplicaSet is looking for the pods constantly in a loop saying, do you have four pods? Do you have four pods? You have three pods? Oh, here's a new pod, and created a new pod for us. And you can see that there in the hello‑world pod ending in pf5vf with an age of 14 seconds. Now, I do want to call out that the pod that we relabeled didn't really change state. Yes, it's outside of the selector for the ReplicaSet, but that pod is still up and running, and so this is a useful technique if we have to debug a running pod.

### Demo: Services, Labels, Selectors, and Scheduling Pods to Nodes

So that's how deployment, replicas sets and pods kind of all pieced together using labels, right? This is the core of how Kubernetes keeps these resources in check and knows if we're in a desired state. Now let's move forward and look at how services use labels and selectors. And so we created that service at the top of this demonstration here with that service.yaml and so I should go ahead and grab kubectl get service to see if everything is good. Yeah, so we see hello‑world service up and running and we have a cluster IP. So, let's go ahead and move forward and bring some additional code into view here and on line 81, I'm asking for some additional information about our service. So kubectl describe service hello‑world, and let's go ahead and look at the detailed information that that gives us. So in here, we see the selector app=hello‑world, and so this individual service is running queries with the selector app=hello‑world and it's getting sets of pods back, right? And so at the bottom there, we see endpoints 192.168.1.105, 106, and so on, right? And so those are the pods that are satisfying the app=hello‑world selector and become registered endpoints of this service when it's up and running. So let's look a little more closely at the endpoints themselves and run kubectl describe endpoints hello‑world to get some more detailed information about the pods that have registered endpoints into this service. In the output here, we can see a set of IP addresses associated with the pods that are registered endpoints for this service. So there we see, 192.168.1.105, 192.168.1.106, and so on. But if we look at the number of registered endpoint IPs, we see that there's five available. Now, I wonder why there is five available if we defined a replica set of four. Well, let's go ahead and look more closely how this all pieces together. I'm going to go ahead and get a list of IP addresses for all the pods that are up and running in our cluster and so we kubectl get pod ‑o wide. We can see the IPs associated with the five pods that are actually up and running but our deployment that we created had a replica set with four pods in it, and we have five. Well, yeah, we created that additional pod when we assigned the pod‑template‑hash to DEBUG, right? It kicked out an additional pod. So let's go ahead and look at the labels for all the pods that are up and running right now. So here we see five that have app=hello‑world. So those are the five that match the label selector for the service. We see four that match for the pod‑template‑hash and one with a pod‑template‑hash of DEBUG. What we need to do now is update the label so that we can kick that one pod out of the selector for the service, right? And so the pod ending in kjvnl is matching because the selector app=hello‑world is finding a match for the label app=hello‑world for that pod, right? And so we want to remove that from load balancing as well. So let's go ahead and grab that pod's name again, put it in our clipboard, paste it into here and update that label. So let's go ahead and run that code to update the label and we can see that kjvnl was successfully labeled. So now, let's go ahead and get a listing of all of our pods and their labels again. So we can see that kjvnl now has app=DEBUG and pod‑template‑hash=DEBUG and this is the state that we'd want to be in if we wanted to remove this pod from operations in a cluster. First, we removed it from a replica set standpoint and the replica set created a new pod for us. But then we needed to remove it from load balancing by updating the label so that it didn't match on the selector for the service. So now if we go back and ask Kubernetes for the detailed information about the endpoints associated with the service, we say kubectl describe endpoints. Now we see that there's only four IPs associated with this particular service. So now that pod is really out of the scope of the service, it's out of the scope of the replica set, and so we can go look more closely at it and debug it if we need to. So that's a wrap for this demo. Let's go ahead and delete our deployment, just to kind of clean up after ourselves when we're done, delete our service, since we're done with that, and go ahead and paste in our pod name here and get rid of that as well. Now for the next demo that we're going to work on, we're going to look at scheduling a pod to a specific node in our cluster, and so let's go ahead and bring that code into view here. Now, we've been working a lot with pods and labeling pods and things like that, but labels can be associated with other resources. In this case, we're going to associate some labels with nodes, right? It's a different resource in our cluster but all resources in the cluster have the ability to get labels attached to them. And so let's go ahead and get a listing of all the nodes in our cluster and look at the labels that are associated with the nodes so far, and so we can see some labels associated with the nodes. We see things like the CPU architecture, we see the operating system is Linux and the host name is specified as a label in our cluster and we also see that the node role for the master is also specified as a label and we see that on c1‑master1. Now let's go ahead and associate some new user defined labels to nodes in our cluster and we can do that here on line 110, kubectl label node C1‑node2 and we're going to assign the label with the key of disk and the value of local\_ssd. We're going to do a similar operation on c1‑node3 specifying the label where the key is hardware and the value is local\_gpu. And at the bottom, we can see that both of those resources or nodes were successfully labeled. So moving forward, we can query our labels just to confirm that they were associated appropriately. So we see kubectl get node ‑L specifying the keys that we want to look at, disk and hardware, and in the output, we can see the values; local\_ssd is associated with node2 and local\_gpu is associated with node3. So let's go ahead and bring this code into view here. Now we're going to create three pods, and two of which are going to be using node selectors and one isn't. So let's go ahead and open up that code. We have PodsToNodes.yaml. Let's bring that into view here, and inside PodsToNodes.yaml, let's look at how we can specify where we want to run these pods. Now we start off just like we would with any other pod creation where we specify the API version, the kind, and so on. But if we look at the spec there, on line 11, we define a node selector. That node selector is then going to define a label with the key of disk and a value of ssd. Now this is what the scheduler is going to look at when it makes its scheduling decision when placing this pod onto a node that has a label that matches this node selector. If we move forward, we can see we have, on line 24 and 25, a node selector for this pod where the key is hardware and the value is local\_gpu. And then we are defining just a regular pod here with no node selector, and we're going to see where that lands in our cluster. So let's go and jump back to our demo script and send this YAML file into our cluster to create those three pods and there we can see at the bottom, pod‑ssd created, pod‑gpu created and the regular nginx‑pod created. So let's go ahead and ask Kubernetes, well, let's figure out where you place this workload. Now, let's go ahead and bring into view the actual labels that we've assigned. So there we can see the labels with c1‑node2 as local\_ssd and c1‑node3 as local\_gpu, and if we use kubectl get pods ‑o wide, that's going to tell us which node a pod is running on. And so in this output here, let's walk through each one of these and see where they landed. All right, so nginx‑pod landed on c1‑node1. That could land anywhere in our cluster because we didn't specify a node selector. Now for nginx‑pod‑gpu, that landed on c1‑node3, and if we look in our output, just above, we see c1‑node3 certainly has the key of hardware and the value of local\_gpu, so that matched and landed on the right node. Now, similarly nginx‑pod‑ssd, that landed on c1‑node2, and if we look in the output above where c1‑node2 has a key of disk and a value of local\_ssd. And so this is how we're able to influence the scheduling of pods or workload into our cluster based on the attributes of the cluster. Not only could it be GPU or SSD, but it could be maybe fault domains in our data center or maybe geographically dispersed data centers depending on how we're distributing workload, and so a very good technique, and this gets us away from pinning things to individual node names, so we can update very easily where these things would live by changing the labels on the nodes and if we needed to move the workload to a different node in our cluster, really not binding it to an individual node name, which would be not really the best practice. So let's get and clean things up. I want to remove those labels from the nodes, and so here we see kubectl label node c1‑node2 disk followed by a minus sign and the same for node3, hardware followed by a minus sign. We're going to remove those labels. Let's go ahead and delete our workload because we're going to wrap up this demonstration.

### Introducing and Working with Annotations

The third organizational method for objects in Kubernetes that we're going to look at today is annotations. Annotations are used to add additional information about your cluster resources directly to the metadata associated with that resource. Annotations are mostly used by people or tooling to help make decisions about what to do with a particular resource based on the annotation or the data stored in the metadata. We'll commonly find things like build, release, or image information attached in an annotation to a particular resource, and this becomes very easily accessible. In addition to these attributes, we also will find things like which team is responsible for a particular resource, or other metadata that helps us give additional information about the resource so that either a person or a tool can make a good decision on that resource. Now, you might be asking why? Like, why would I want to store this additional metadata or this annotation along with the individual resource deployed in a cluster? Well, what this does is it saves you from having to write integrations to retrieve information from external data sources and then merging it together with the information that you're getting from Kubernetes. It simply all lives in one place inside your cluster attached as an annotation to the resource that you want to operate on. Now, an annotation is a non‑hierarchical key/value pair, so in that sense it's very similar to a label, right? We have a key and we have a value. Now, a big difference between annotations and labels is that annotations can't be used to query or select resources, right? We can't query and write a selector to pull out a subset of pods or resources on an annotation. We have to use labels to do that. The data that's stored in annotations is used for other purposes. It's used by people or tooling to help make decisions on what to do with a particular resource if we needed to perform an operation on it or help identify or gain more information about the resource that we're working with. Now, similar to labels, annotations, the keys can be up to 63 characters in length. But very different from labels is that an annotation can have a value that can be up to 256 kB in length, so a lot of information can be stored in a value of an annotation. So when it comes to adding and editing annotations to resources in Kubernetes, let's look at how to do that both declaratively and imperatively, and so we'll start off with a declarative implementation. At the top here, we start off by declaring our resource just like we would any other resource, so apiVersion, kind. In this case, we're going to work with a pod. Well, we can attach an annotation to any type of resource inside of Kubernetes. Then we go ahead and specify the metadata. Here we'll specify the name, in this case, nginx‑pod. Now here's where things get a little bit different. We specify the annotation and then the key and the value pair, the key being owner and the value being Anthony. And then we go about specifying the remainder of the information for this particular resource that we want to deploy. Now imperatively at the command line, it's going to look like this, kubectl annotate, then the resource type pod, and then the resource name, nginx‑pod. Then we specify the key and the value pair, so here we see owner=Anthony. If I want to make an edit to that annotation, here's what the syntax looks like for that. So, kubectl annotate pod nginx‑pod. I'm changing the value for the key owner from Anthony to NotAnthony, and then for this to overwrite the existing value in that annotation, we'll have to specify ‑‑overwrite. Now, let's take some time to review these organizational methods that we talked about for organizing objects in Kubernetes. And first up is namespaces, you'll want to use a namespace when you want to put a boundary around resources, security, or naming inside of your cluster. Basically, subdividing your cluster into virtual clusters and then grouping objects based on those concepts. Then we looked at labels. We're going to want to use labels when you want to act on a collection of objects in your cluster. This gives you the ability to query and retrieve collections of objects and perform some actions on those. We can also use labels to influence the internal operations of Kubernetes, whether it be from controllers, services, or scheduling. And then finally, we looked at annotations. You're going to want to use annotations whenever you want to add additional information directly to a resource, and that information will live with that resource inside of the cluster.

### Module Summary and What's Next

Well, here we are at the end of this module, and let's review some of the key topics that we discussed. We talked about organizing objects in Kubernetes and the three primary ways that we can do that, with namespaces, labels, and annotations. Then we looked at how Kubernetes uses labels, specifically in the scenarios with services, deployments, and scheduling. Well, with that, that's a wrap for this module. Join me in the next module, Running and Managing Pods.

4

Running and Managing Pods

1h 25m 56s

### Introduction, Course, and Module Overview

Hello, this is Anthony Nocentino with Centino Systems. Welcome to my course, Managing the Kubernetes API Server and Pods. This module is Running and Managing Pods. So far in this course, we took a deep dive into the Kubernetes API, and then we also looked at how to manage objects in our Kubernetes clusters with labels, annotations, and namespaces. Now, let's move the conversation towards running and managing Pods in our cluster. Now, it's time to get into that fundamental workload element and learn how we can run and manage Pods. In this module, we're going to start the conversation off with understanding pods, and why do we need this abstraction of a Pod around our container‑based application? Then we'll look at the interoperation between controllers like deployments, and ReplicaSets, and Pods themselves and learn why we need such a construct. Next, we'll look at multi‑container Pods where we have multiple containers resident inside of a single Pod and why we would use something like that in our container‑based application deployments. And then we'll wrap up the conversation with managing Pod health with probes, where we'll give Kubernetes a little more information about the health of our application so that it can make good decisions on how to react in certain scenarios with regards to our applications that we're deploying in Pods.

### Understanding Pods

So what exactly is a Pod? Well, a Pod is a wrapper around your container‑based application. It's the thing that will deploy into our Kubernetes cluster, and it's the job of Kubernetes to figure out how to run that workload in the cluster with this abstraction that is a Pod. Most commonly, there will be a single container per Pod, but there are many application scenarios where you'll deploy multiple containers within a Pod and then deploy that workload into your cluster. Now, in addition to the container‑based application, a Pod also has resources, the resources associated with the execution environment that is your application. So things like storage, networking, and application configurations with constructs like environment variables, that's all wrapped up in this thing that's called a Pod that we deploy into our cluster. The Pod is the unit of scheduling in Kubernetes. In fact, it's the responsibility of the scheduler to figure out where to allocate Pods into the cluster, onto the nodes based on the resources that are available. But when a Pod is up and running in our cluster, it's simply a process that's running somewhere consuming resources in our cluster. The Pod is also the unit of deployment, so it defines the application configuration and the resources associated with the Pod, such as storage and networking. So why do we need Pods in Kubernetes? Pods provide a higher‑level abstraction over our container‑based applications for manageability reasons. We've learned so far about scheduling and how Kubernetes uses that to deploy our applications into our cluster, but Pods also provide an abstraction around the execution environment for our container‑based applications in terms of configuration and resources such as networking and storage.

### How Pods Manage Containers

So how do Pods manage containers? There's three primary ways that this happens, and first up is single container Pods. This is where we have a single application container wrapped up in a Pod and deployed into our cluster. This is the most common deployment pattern for getting container‑based workloads up in Pods inside of Kubernetes. Now, there are some scenarios where we'll have multi‑container Pods, where there's more than one container wrapped up in a Pod and deployed into our cluster. And we'll find these deployment scenarios are for tightly coupled applications that have some sort of producer/consumer relationship. And then next, there are init containers. Init containers are containers that run before the main application container is started in a Pod, usually used to set something up for the application container before it starts. We're going to look at each of these deployment scenarios more closely in the upcoming slides.

### Introducing and Working with Single Container Pods and Controllers

Single container Pods, as we mentioned before, are the most common deployment scenario. And generally speaking, when you have a single container Pod, you're going to have a single process executing inside of that container. And so what this translates to is a Pod running a container running a single process on a node in our cluster. And this often leads us to easier application scaling because we're minimizing dependencies between our applications by running just a simple process inside of a container. This allows us to scale our application out by adding more Pods, which leads to more containers, which leads to more processes, really driving home and fulfilling that microservices philosophy. Let's look at the relationship between controllers and Pods. Controllers have the responsibility of keeping your applications or your Pods in the desired state. Controllers have the responsibility of starting and stopping Pods based on whatever configuration you assert upon your cluster. If you tell Kubernetes you want four replicas of a ReplicaSet, it's going to deploy four Pods in your cluster for you. Now in addition to keeping you in the desired state, controllers can also be used for application scaling. If you have the resources available in your cluster and you want to grow from 4 to 40 to 400 Pods, controllers can make that happen for you very easily based on the configuration that you assert upon a controller. If you tell it you want to increase the number of Pods in your application, it will do so for you. In addition to application scaling, controllers can also facilitate for application recovery, again, keeping things in a desired state. So if for whatever reason a Pod goes offline or becomes unavailable, Kubernetes can sense this and will recover and make sure that you have the correct number of Pods up and running in your cluster. Now, I want to make a key point here is that you don't want to run bare or naked Pods in Kubernetes, generally speaking. What a bare‑naked Pod is is a Pod that's deployed and not under the control of a controller, such as a deployment, a ReplicaSet, or a DaemonSet, or other type of controller that's available in Kubernetes. And the reason why you don't want to run a bare or naked Pod is that they won't be recreated for you in the event of a failure. If you have a Pod that's up and running on the node and that node dies, well that Pod won't be recreated for you somewhere else in the cluster. It's simply going to go away.

### Introducing and Working Static Pods

Now that we've introduced Pods started by controllers and bare Pods, let's introduce one more way to start Pods in our cluster, and that's static Pods. Static Pods are Pods that are managed by the kubelet on specific nodes. To create a static Pod, you'll create a static Pod manifest, which is just a YAML manifest describing a Pod, and then you'll save that manifest into a location on the file system that the kubelet is watching. That file system location is called the staticPodPath, and by default, that location is /etc/kubernetes/manifests. So you write a Pod manifest and save it into this location, and the kubelet on that node will read the manifest and start up the Pod on that node. We learned a little bit about static Pods and static Pod manifests in the first course in a series of Kubernetes installation and configuration fundamentals where we learned that this is how the control plane Pods are started when the control plane node starts up. Those static Pod manifests were created by the kubeadm bootstrapping process. Static Pods can be used to start up any Pod, not just control plane Pods. The staticPodPath watched by the kubelet is configurable, so if needed, you could change that setting by editing your kubelet's configuration. The kubelet's configuration is in /var/lib/kubelet/config.yaml. The staticPodPath is watched by the kubelet, so if you reboot a node, any manifests in the staticPodPath will be processed when the kubelet starts up, and those Pods will be started. Further, if you make a change to a Pod definition in a manifest, that will be affected on the Pod's deploy. For example, if you update a container image in the manifest, this will cause the kubelet to restart the Pod with the new container image. If a Pod fails, it'll be restarted by the kubelet. And finally, when you've removed the static Pod manifest from the staticPodPath, the Pod will be terminated and deleted. The static Pods that are created using this technique are controlled by the kubelet, not the API server. But the kubelet will create what's called a mirror Pod for each static Pod. This means you'll be able to see the Pods from the API server with commands like kubectl get pods, but you will not be able to control the Pods via the API server. You must interact with it by managing the static Pod manifests in the staticPodPath.

### Working with Pods - kubectl exec, logs, and port-forward

So now let's look at how we can work with some Pods. We've already covered in great detail how to deploy bare Pods and Pods under certain types of controllers into our clusters. So we're going to skip past that, but if you need a review of that, check out Kubernetes installation and configuration fundamentals or the other parts of this course where we covered the creation of Pods in some of the other demonstrations. For this, we're going to look at how we can interoperate with Pods and ask our Pods for additional information when it comes to troubleshooting and other techniques for access. And so, let's say we have a Pod that's up and running in our cluster, and administratively, we're sitting at our console. We need to interact with this in some way. Let's say I want to execute a process or something like that on that Pod to just interrogate it more closely. Let's say we wanted to start up a bash shell on the container running inside of that Pod, and so here's how we'd go ahead and do that. So kubectl exec ‑it, then the Pod name, in this case, POD1. If it's a single‑container Pod, we don't have to specify the container name, but if it's a multi‑container Pod, we should, and we would do that with ‑‑container and then specifying the container name. And then we follow that up by ‑‑ /bin/bash, and that's going to be the process that we're going to execute on that container in our application. And now let's look at what happens behind the scenes when we execute this command. When we execute kubectl exec, it's going to open a connection up to the API server. Then that API server is going to open up a connection to the kubelet on the node that's running that Pod and attach a process to the container running inside of that Pod, and that's going to allow us to execute this program, and the output that we generate will be streamed back across that same socket to our kubectl command running on our localhost. Now similarly, this is the same process that happens when we do kubectl logs and we go ahead and we want to get the log from that remote Pod. So again, the same flow. We open up a connection to the API server, the API server connects to the kubelet, reads that log information out, and streams it back to our kubectl process. Now I want to explore another scenario that we haven't covered so far in this course. What if I need to talk to that application that's running on that Pod and interrogate it to see if it's running the application correctly that we need to interoperate with? And now, since that Pod might be on a Pod network, it's not really accessible to me if I'm running remotely maybe in a cloud scenario where I'm on‑prem and my Pod is up in the cloud somewhere and I dont have direct network access, but I can reach the API server. Well, we can just kubectl to help us and access that container‑based application, and this is how we can go about doing that, kubectl port‑forward. We specify the resource you want to port forward to, in this case, a pod, and then the Pod name, POD1. We specify a local port, which is going to be instantiated on the local system that we're running kubectl on, and then we specify a colon, followed by the CONTAINERPORT, which is the container‑based application. In this case, in this example, it would be port 80. We send our traffic into that local port, and it pops out the other end inside of our Pod on the port that we requested with our container port.

### Demo: Running Bare Pods and Pods in Controllers

So here we are in our first demo for this module, we're going to start off with running pods. We'll look at bare pods and also creating pods in a deployment. Then, once we have some pods up and running, we'll look at how we can use port‑forward to access a pod's application when we don't have direct access to the network that the pods are running on. And to wrap things up, we'll create some static pods. Alright, so here we are in VS Code with the session open to c1‑master1 on the bottom here, and the first thing that we're going to do is we're going to start up a kubectl events with the ‑‑watch parameter and background that, because I want to be able to show you the events that occur when I create some pods and delete some pods, and also create deployments and scale it up and scale it down, and we'll see what Kubernetes does behind the scenes to make that happen for us. And so, let's go ahead and start off by creating a bare pod. Inside pod.yaml I have the basic pod definition to create a bare pod for us, and let's go ahead and push that into the API server and see what happens. So in the bottom here we see Scheduled Pod Successfully assigned default/hello‑world‑pod to c1‑node3. The default there is we deployed this pod into the default namespace. The next line we see Pulled Pod Container image and then the name of the container image, and it says already present on machine. That means that this container image is already present on that machine from a previous time when I had to pull that container image from the container registry when I started this workload in other demos. If that container image wasn't already there, it would be the responsibility of the container runtime to go ahead and pull that container image from its container registry. The next line we see Created Pod Created container, and then Started Pod Started container, and so now, our single pod is up and running and our application should be up and running as well. Now let's do something similar with the deployment. Inside this deployment.yaml, I have a simple single replica deployment that runs our hello‑ world application, so let's go ahead and run that and watch the events that occur when we do this. So here we see ScalingReplicaSet Deployment Scaled up replica set hello‑world, and then the pod template hash, so, ‑5646fcc96b, and at the end there we see scaled it to 1, and it kind of wraps onto the new line there. Now, we also see SuccessfulCreate ReplicaSet, Created pod: hello‑world being our deployment name, then the pod template hash, and then a unique identifier for that individual pod, which is dk4mm. The next line we see that that pod is successfully assigned to c1‑node1, and then we go through that process again that we did earlier when we created the bare pod where on that node it checks to see if the container image is there, it is, so it didn't have to pull it, then it goes and creates the container and starts the container, and so that deployment is up and running on c1‑node1 now. So let's go ahead and see what happens when we scale our replica from one to two. We can do that with this command here, kubectl scale deployment hello‑world ‑‑replicas=2, run that code, and we'll see a very similar process that we saw when we created the first replica in our replica set. So here we see ScalingReplicaSet Deployment Scaled up replica set hello‑world, then our pod template hash to. Then we see SuccessfulC ReplicaSet Created the pod, then we get a new pod, hello‑world, followed by the pod template hash, and then a new identifier 8pgfl. Next line shows us that it's scheduled to c1‑node2, looks for the container image, which is already there, creates the container and starts the container. It's a very similar process to go from 1 to 2 as it was to go from 0 to 1. Now, let's go ahead and scale our deployment down from two replicas to one, run that code right here, and we can see Kubernetes events tells us ScalingReplicaSet event has occurred, for the Deployment, Scaled down the replica set to one. We see it deletes the pod in the next line there, and then it goes ahead and tells that node to kill that container and shut our application down. So now if we do kubectl get pods, we should see two pods, one associated with our deployment and one in that bare pod that we deployed initially. So if we keep moving forward, let's go ahead and bring this code into view here, and for this one, let's go ahead and we're going to instantiate or execute a process on a container in a pod. And in this case, the process is going to be /bin/sh to give us a shell onto that remote container running in the pod on that node. So, let's go ahead and use this one here, and we'll go ahead and copy and paste this pod name that ends in dk4mm and paste it here into this line of code. Let's go ahead and walk through this line of code. So we're going to do kubectl ‑v 6, we're going to increase the verbosity so we can see the API transaction between kubectl and the API server. We'll have exec, because that's the operation we're going to perform, ‑it for integrated terminal, and then the pod name ending in dk4mm. We have a ‑‑ /bin/sh, which is the process that we want to run on our container in that pod. Let's go ahead and do that, and we see here at the bottom we have a hashmark or a prompt available to us at the very bottom, and then let's go ahead and look at the API requests that occurred when we did this. The first API request checks to see if the pod's there. So we see a GET against the full API path to that individual pod and we get a 200 OK. Then we see a POST operation against that pod as well, but we have some additional attributes associated with the API transaction, so in that operation that we posted to that resource, we see exec?command= and then %2F, which is a forward slash, bin/sh and container=hello‑world, and so on. So this is how we instantiate the process on that remote container with kubectl exec. Let's go ahead at the bottom here and we can do a quick hostname to see that this certainly is the pod that we're connected to because the hostname is the pod's name. If we do a ps, on the container we can see our hello‑app is PID 1, our shell is PID 9, and this ps that we just executed is PID 16. Now, let's go ahead and exit out of this container and get back onto c1‑master1 with the exit command. And so that's our container‑based application. It's simply a process running in a container, running in a pod on a node in our cluster.

### Demo: Running Pods and Using kubectl port-forward to access a Pod's Application

And so we just looked at the process listing inside the container, let's look at what it looks like outside the container at the node level. To do that, I need to know where that container‑based application is running, and we can ask Kubernetes for that information with kubectl get pods ‑o wide, and we can see that our pod is running on c1‑node1. And so let's go ahead and open up an ssh connection into c1‑node, and with that, now I have a connection onto c1‑node1, and if I do a process listing, I'm going to see all the processes that are executing on c1‑node1. So let's go ahead and run that here and filter for the process name for our container‑based application, and there we see hello‑app in the process list with a PID of 63151. Our container‑based application is just a process running on a node in our cluster, and it's the job of Kubernetes to figure out where to run that workload, make sure the execution environment is set properly, and also adding any additional resources, such as networking, or disk, or whatever it is we need to run our application in that pod. And so in the end though, it's just a process running in a container in our cluster. So, let's go ahead and exit out of this and get back onto c1‑master1. Go ahead and move forward in our code a little bit, and now I want to be able to access our pod's application directly. We didn't front end this application with the service and let's say that we weren't on the pod network at all, and so we didn't have any network reachability to the pod that's executing inside of our cluster. We can use kubectl port‑forward to actually access the application running inside that pod, and so let's go ahead and do that here. In this command, line 33, we say kubectl port‑forward, and I'm going to go ahead and put our pod name right here, and then we're going to use a local port of 80:, and the container port of 88, and so that's going to go ahead and open up a connection from my local system on port 80 all the way through to the container port on port 88. So let's go ahead and run that code here and see, uh‑oh, that's, oh, I know what happened, let's see what happened here. So we executed kubectl port‑forward against our pod and we specified port 80, which in Linux is a privileged port. So anything below 1024 we would need to run this command as root to be able to get access to the privileged port. In this case, the better solution than running this with pseudo or with privileged access is to make a modification to the local port, which we're executing on, I just so happen to have that code right here. So here we see kubectl port‑forward, let's go ahead and paste in the pod name, and I'm going to go ahead and run that application on 8080 locally, which is a non‑privileged port, and then relay that to the container port on 8080, and I'm going to throw an ampersand on the end there to background this task. So here we can see at the bottom, we're forwarding from 8080 to 8080. Now with that, I should be able to use curl to access our container‑based hello world application running on 8080 in the container port by making a request of http://localhost:8080, which is my local port here. I run this code here, and I certainly see I get the output from our container‑based application, we see Hello, World!, version 1.0.0, and then the host name, which is also the pod name, hello‑world, there's our pod template hash followed by the pod ID, which is dk4mm. So let's go ahead and kill our port‑forward session with an fg here, and kill it off with a Ctrl+C, and the final two lines of code here are to clean up after ourselves, so let's go ahead and delete our deployment and delete our pods, and it looks like I still have that watch running, so let's go ahead and bring that into the foreground and kill that off.

### Demo: Working with Static Pods

Now for our final demo in this series of demos, let's look at static pods. And so what we're going to do here on line 55 is create a pod manifest using kubectl dry‑run combined with the ‑o yaml parameter to generate a syntactically correct pod manifest very quickly. And so on line 55, we have kubectl run hello‑world, which is going to be the name of the pod, and then we're specifying the image to be hello‑app with a tag of 2.0 from the Google Container Registry, our sample hello‑world application that we have been using. We're going to combine that with the dry‑run=client parameter and ‑o yaml, and then specify the container port as 8080. And so when we run this code here, what we'll get at the bottom is our pod manifest. And so I'm going to go ahead and take all of that code and throw that into my clipboard, because what we're going to do now is take this code over to c1‑node1 and create a static pod manifest. And so let's start that process. I'm going to SSH into c1‑node1, and before we create the static pod manifest, I want to show you where you can look in the kubelets configuration to determine where your static pod path is. And so let's take a peek at the kubelets configuration file, which lives in /var/lib/kubelet/config.yaml. So here it will cat that out to the console, and in the output at the bottom, we can see staticPodPath: /etc/kubernetes/manifests. So that's where we need to create our static pod manifest, and so let's start that process. On line 65, I have sudo vi /etc/kubernetes/manifests, which is our static pod path, and I'm going to give our static pod manifest a name of mypod.yaml. So let's go ahead and open up an editor to that, and switch over into insert mode, and paste in the pod manifest that we just created together at the command line. Jump up to the top of the file, and here we can see that this is a pod manifest. Let's save this file out, and take a quick look at a directory listing in the static pod path to see if our file was there, and there is my pod.yaml. So let's jump out of c1‑node1 and get back on the c1‑cp1, and now if I do a kubectl get pods ‑o wide, we'll see that I have a pod up and running. Now if we look at the name of the pod, we can see it's in the format of pod name and then a dash and then a node name. So we have hello‑world‑c1‑node1. Now, even though we see this pod in the output of kubectl get pods, it's not an actual pod in the API server, it's a mirror pod. And so if we try to delete the pod, like we're going to do here on line 76, well, it's not really going to delete the pod, it's just going to delete the entry for the mirror pod in the API server. And so when I run kubectl delete get pods hello‑world‑c1‑node1, it'll tell us that the pod is deleted. I'll come back a few moments later, and we'll see that the mirror pod gets recreated. So there we see hello‑world‑c1‑node1 is back up and running again. This actually had no impact on the running lifecycle of the pod on c1‑node1‑, just the mirror pod. Now, if I jump back on to c1‑node1, and if I delete the static pod manifest from the static pod path, the kubelet is going to sense that that file is no longer there, and then it's going to terminate and delete the pod. Now let's hop back out on to c1‑node1, and if I do a kubectl get pods now, we can see that there are no resources available in the default namespace. Our static pod has been terminated and deleted.

### Introducing and Working with Multi-container Pods

So far in this series of courses, we've focused exclusively on single‑container Pods. Let's look at multi‑container Pods, how they work, and why you would use them. The primary reason why you'd use a multi‑container Pod is if you had very tightly‑coupled applications. And the reason is because those processes in those multiple containers will be scheduled together onto the same node because the unit of scheduling in Kubernetes is the Pod, so those containers will be deployed and instantiated on a node in a Pod. Now the key reason why you'd use a multi‑container Pod is there's likely some requirement on a shared resource between the containers running inside of that Pod. And usually, what this looks like is one of the containers is generating data while the other one is consuming data. It's some sort of producer‑consumer relationship. Now, this isn't an exhaustive discussion on design patterns for multi‑container Pods. In fact, there will be some course work on that in the future. But this is just the big picture idea when you'd want to use multi‑container Pods, and this goes into your application architecture and design. Now, I do want to call out is that we don't want to use multi‑container Pods to influence scheduling of Pods in our cluster. We use other techniques for that. In fact, we learned that technique back in the module, Managing Objects with Labels, Annotations, and Namespaces. We can influence the scheduler using labels on our nodes and node selectors in our Pod definitions. The YAML code for the manifest for multi‑container Pods is very similar to what you worked with when working with single‑container Pods, and so let's walk through the code together. Just like any other resource in Kubernetes, we start off with our API version and specify the kind, in this case, the Pod. We're going to specify some metadata and give it a name. Our name's going to be multicontainer‑pod. Now in the Pod spec, you'll notice before that the word containers was plural because we can list multiple containers in this part of the Pod spec. And it's also a requirement that the containers have unique names. So here we see name, nginx, we specify the image, nginx. and the ports for the containerPort and so on for that particular container in this Pod. If we need to fill any additional information about that container, we'd specify it here, and then we'd go and specify the next container in the Pod. In this case, its name will be alpine, and the image will be alpine. Now when working with customers or talking to conference attendees, I get a common question about designing multi‑container Pods, and so let's go ahead and review a common anti‑pattern for multi‑container Pods. This comes up very frequently in both of those scenarios. When deploying multi‑container Pods in Kubernetes, what you don't want to do is something like this, pair the web server and the database server inside of a Pod. On the surface, this might make sense. They're co‑resident. They can very efficiently exchange data. But in operations, this isn't the right way to get things done. And we're going to look at this from two different perspectives, the first off being recovery. What are my recovery options to bring this Pod back online if I have a stateful database server paired with a stateless web server? What type of controller would I use? Would I want that controller to just kill this Pod and recreate it? I need to take that into account in my Pod design. Now we're not going to focus too much on controllers in this course. We're going to pick that conversation up in a later course, and there's ways that we can deploy database servers and stateful applications into Kubernetes. But when we pair these two things together, it really limits our ability to wrap a controller around a Pod. Now the second issue with this anti‑pattern for multi‑container Pods is this really limits scalability. Now, the unit of scaling in Kubernetes is the Pod, and so I'd have to scale these two containers together, as I added additional Pods. And, well, web servers and database servers certainly have two very different scaling patterns when one is stateless and one is stateful. Within a Pod, we talked about how there are shared resources, specifically in terms of networking and storage. And so let's take a little time to look at these both more closely. And let's start this conversation with networking. Containers in a Pod share the same Linux operating system level namespace. This isn't the Kubernetes namespace that we introduced previously in this course. Since they share the same network namespace, to be able to communicate to each other, these two containers must communicate to each other over the loopback interface inside of their container. If these containers need to talk to other Pods on the network or other network resources, they would simply reach out over IP like they would to open up any other network communication. Since they do share the same network namespace, we need to be mindful of application port conflicts. And so these two container‑based applications, when they open up network ports, they need to be unique network ports within this Pod due to the shared network namespace. Now let's look at shared resources inside of a Pod from a storage perspective. Each container image within a Pod in a multi‑container Pod scenario will have its own file system. When we define volumes or persistent volume claims, they're going to be defined at the Pod level, which means these are shared resources amongst the containers in a Pod. Now this shared storage resource can be mounted into the containers' file system, which makes this an accessible resource to any container that mounts it within this Pod. This becomes a common way for containers to exchange data, and we're going to see this in action in an upcoming demo.

### Demo: Running Multi-container Pods and Sharing Data Between Containers in a Pod

Alright, so in this demo we're going to look at running multi‑container Pods and how to share data between containers in the Pod at the storage level. Alright, so here we are with VS Code open again, and our code loaded up on the top here, and we have a terminal session to c1‑master1 at the bottom. The first thing that we're going to do is look at this multicontainer‑pod definition that I built, so multicontainer‑pod.yaml. So it starts off looking like any other Pod definition. We have our API version, we have the kind set to Pod, and the metadata giving it a name, which is multicontainer‑pod. If we go down a little bit further, though, we see this is where things get different. So in the Pod spec under containers, we're specifying two containers. So at line 7, we have name: producer and at line 14 we have name: consumer, and those are the two container‑based applications that will be resident inside of this Pod. Now the first container, producer, is going to run a container image ubuntu, and our second container, consumer, is going to run a container image named nginx. So you can imagine a scenario here where the ubuntu container is going to be producing some information in its container‑based application and our nginx, or our consumer container, is going to be reading that information and displaying it via its web application, that is nginx. If we look at the producer container on lines 9 and 10, we're instantiating a bash shell, and then running a while loop on line 10 that's basically going to echo the hostname and the date on this particular container, and write that information out to var/log/index.html. Now, let's go ahead and jump to the bottom of this definition. Here we see volumes on line 21, and we're specifying a volume with the name of webcontent and of type emptyDir. EmptyDir is a volume type where data is written locally on the node. We can use this Pod‑level resource to expose this volume to both of the containers in our Pod. We're going to use this as our shared volume, so these two containers in this Pod can easily exchange data. But I do want to call out, know that when this Pod goes away, the data written to emptyDir will go away as well. There is no persistency here. This could be any type of persistent volume inside of our Kubernetes storage universe if we wanted to, but for this demonstration, it works well to use emptyDir mounted into each container‑based application. And so let's look at how we mount that volume into our container, so let's go ahead and bring that code into view here. In the producer container on line 11, we declare volumeMounts. We're going to mount the volume name, web content, which we declared below on line 21, and we're going to mount that into the mountPath var/log, as you can see here on line 13. So anything that happens inside our container which reads or writes from that mounted volume in the file system at var/log, is going to perform that action on the volume, web content, which is that emptyDir‑backed volume. In the consumer container, we have volumeMounts, and again, we're going to use the same name, webcontent, and a mountPath of usr/share/nginx/html. That's the default location in which nginx reads from. And since we're writing out index.html file in the producer container, when we go access the web application in our nginx container, it's going to read that index.html and display that information when it's requested from the web server. So let's jump back over to our code and go ahead and deploy this application, and we do that with kubectl apply, and we're going to feed in multicontainer‑pod.yaml. And there we can see multicontainer‑pod.yaml was created. And so let's go ahead and bring this additional code into view, and I'm going to open up a kubectl exec to multicontainer‑pod, and open up a shell with bin/sh. Now, when we don't specify a specific container, kubectl exec will default to the first container in the container configuration. So there we can see in the first line of output there it says defaulting container name to producer, right? That's the producer of the information, and that's our ubuntu container that's generating our web application data. If I do a directory listing on var/log, which is where we mounted that shared volume, we'll see we have an index.html that was created just a few seconds ago. I'll go ahead and type in date here. We can see that we're just a little bit off of that from a few seconds ago on that date and time. If I look inside of the file, it's got a tail at the end of that file, and let's see what's inside of there. We can see what we're doing is we're writing the pod name followed by the date in a loop every 10 seconds. So let's go ahead and exit our shell on that container and get back to c1‑master1. And let's go ahead and bring this code into view on line 19 here, and go ahead and log into the consumer container. So from left to right, let's look at the code to do so, kubectl exec ‑it. There's our Pod name, multicontainer‑pod, and I'm specifying the container that I want to log into, which is ‑‑container, followed by the container name consumer, then ‑‑bin/sh to open up a shell. Now I have a shell on the consumer container. Now this is an nginx web application, and we mounted our shared volume into usr/share/nginx, so let's get a directory listing inside of that directory. And here you can see index.html, and we can see that the index.html was created just a few seconds ago as well. If we use tail to read the contents at the bottom of this index.html file, we can see it's still being updated every 10 seconds, so it's the same exact file between the two containers. Let's go ahead and exit out of the shell on our container and get back to c1‑master1. We're going to use that port‑forwarding technique to access this container‑based application so we can see that nginx content. This is deployed in a bare Pod, and we didn't front end it with a service, so we'll use this technique to access our container‑based application. So the syntax for this is going to be kubectl port‑forward, multicontainer‑pod. Our local port will be 8080 and the container port will be 80. We'll put an ampersand on the end there to background that task. And if we try to access that container‑based application with curl http://localhost:8080, we'll see the index.html file, which is constantly being updated every 10 seconds with new content.

### Introducing and Working with Init Containers

Now, let's take a look at a variation of a multi‑container Pod, a Pod with init containers. Init containers are containers that will run before the main application container in a Pod is started. Init containers are commonly used to run tools or utilities to set up an environment for the main application container in a Pod. An init container must run to completion before starting the main application container, meaning the init container will perform some sort of task, and then when that application successfully finishes its execution, it's going to change the container state to completed, and then the main application container is started. You can have multiple init containers in a Pod, each of which are run sequentially, and each init container is run in the order that they're specified inside of the Pod spec. And then when all init containers have successfully completed, the main application container can start up and the Pod reports its status as ready. If one of the init containers fails, it will stop executing and not run the remaining init containers, nor will it start up the main application container. And so it's important to note that all of the init containers need to run to successful completion for the main application container to start up and for the Pod to report its status as ready. When an init container fails, the container restartPolicy applies. The container restartPolicy allows you to control what the container does in the event of a failure of the application running inside of that container, and by default, the kubelet restarts failed containers. There are other container restart policy settings, which we'll explore later on in this module. When building applications that require init containers, there are some common patterns, and so let's look at a few. And first up, we've already introduced that you can use init containers to run tools or utilities to set up the execution environment for your main application container, the primary benefit here being the tools don't have to be part of your main application container image. Now this leads us to our second pattern, separation of duties. Using init containers, the init containers can run potentially sensitive operations at higher privilege levels. This means your main application container can run at a least privilege level for its main functions. And then finally, init containers can be used to block container startup, meaning you can use an init container to ensure some sort of environment settings are in place before starting up your main application. Let's look at what it takes to create a Pod that uses init containers, and just like any other Pod definition, we'll start off with an API version and a kind, which is Pod. And now we're going to skip down into the Pod spec, which is where we'll define our init containers. Each init container will have a name and an image that we want to run. And so here you see the name is init‑ service and the image is ubuntu. And then you'll define a command that you want to run. So here in our example we're going to launch a shell that's going to print the standard out that we're waiting for the service to start, and then sleep for 2 seconds, simulating a task that needs to run to completion. Then we'll use that same pattern again if you want to define another init container, and so here we see name: init‑databases, image: ubuntu, and then a command that says waiting for database and then sleeps for 2 seconds. And then finally is the main application container. And so in this example here we can see that the name is app‑container and it's going to run the image nginx for the main application container. And so when this Pod is created, the init containers are processed one after the other, waiting for each to successfully complete. And so init‑service will echo out, wait for 2 seconds, and then init‑database runs, echoes out, and then waits for 2 seconds, and then finally, our main application container will start up. If any of these init containers fails, it will not proceed to the next init container, nor will it proceed on to start up the main application container in the Pod.

### Demo: Working with Init Containers

Alright, let's get into a demo and see init containers in action. Let's get started with Init‑Containers. Now, the first thing that we're going to do is launch kubectl get pods with a watch, so that we can watch the progress of our init containers, and then also the main container start in the Pod that we're deploying here. And so let's go ahead and do that with the code on line 7, kubectl get pods ‑‑watch, and then we're going to throw that process in the background with an ampersand. And so, what we'll do next is to create a Pod with two init containers, and so let's take a peek at the YAML manifest to do just that. Now, in the code here, this is the code that we walked through in the presentation portion of the course, and so what we'll have is two init containers. So here we see our initContainers, init‑service and then init‑database. Once those two finish in sequence, then our application container, app‑container, will start up and run our nginx image. So let's jump back over to our driver script here and deploy that manifest to the API server with kubectl apply ‑f init‑containers.yaml, and we'll run that code there to start the creation of that Pod. And so in the output at the bottom we can see the various state changes of our Pod as it goes through the startup process running each of the init containers and then starting up the main application container, and so let's walk through some of that output at the bottom. Now the first status we see there for our init‑containers Pod is Pending. That means the Pod is getting scheduled, and created, and starting up. Now, on the next line we see the status change from Pending to Init:0/2, and what that indicates is that we're at the part of the process that's starting the first init container, and so there we see 0 of 2 init containers has completed. So right at that point in time for each of those iterations where we see 0 of 2, that first init container is running. Once it completes, we see the status change from Init:0/2 to Init:1/2. One init container has completed. And then once the second init container finishes, the status switches to PodInitializing. Now that main application container is starting up. Then we see the status switch to Running. And then in the Ready column we see 1/1. That means our one application container is up and running inside of our Pod. And so with that, let's go ahead and break out of our watch, bring that into the foreground and kill that task, and we'll clear our console to start off fresh. Now we're going to take a look at this init container from another angle. We're going to use kubectl describe pods to help us look at some detailed information about the pod that ran some init containers, and so let's do that. On line 18, I have kubectl describe pods. Which Pod do I want to describe? That's going to be init‑containers, and then we'll pipe that output into more. And so we have the standard output here for kubectl describe pods, and so we see the name is init‑containers, and we see the current status of the Pod is Running, and then the Pod IP. Going down in the output here, we have a field for Init Containers, and then under that we'll have each init container defined. And so the first one we see is init‑service, which was the first init container that we ran. Looking at the output here, if we scroll down a little bit, we see the State is Terminated and the Reason is Completed. This init container ran to successful completion and then terminated. Let's go down a little bit further. We see the second init container, init‑database. It too is terminated because it completed, and we can see that in the output, State: Terminated, Reason: Completed. If we keep going down in the output, we then have our main application container in the Containers field, there is the app‑container. Its state is running, because, well, the other two init containers finished their processes in sequence successfully, and then it started up the main application container, which is nginx, which is a daemon, and so that's going to continue to run, so the State there is Running. Let's take a peek at some more information about our Pod with init‑containers. If we go down to Events, we'll kind of see that same flow right. In the Events here, we see it's successfully scheduled to c1‑node2. It pulled the image for ubuntu. It then created the container init‑service, started the container init‑service for that first init container. Then we see the same pattern again for the second init container, init‑database. And then finally, our main application container, the image gets pulled, the container is created, and then the container is started. And so with that, let's go ahead and clean up from this demo and delete the Pod with init containers with kubectl delete ‑f, and then we'll specify the manifest that we want to delete, init‑containers.yaml.

### Pod Lifecycle, Stopping/Terminating Pods, and Persistency of Pods

So far in this module, we've seen little bits and pieces of Pod lifecycle, especially when we looked at in the demonstrations using kubectl events. And so let's take some time to look more closely at Pod lifecycle now. At the highest level, Pod lifecycle is going to pass through three phases: creation, running, and termination. Now let's look at more closely how a Pod transitions between these three phases. When working with Pods, they're going to be created in one of two ways, administratively or by a controller. So administratively at the command line, we'll tell Kubernetes to do something, create a Pod, or we'll create a controller which has the responsibility of creating Pods for us. Once our Pods are created, they transition into the running phase and are scheduled onto nodes, and get our Pods up and running in our cluster. As things go along in the lifecycle of a Pod, eventually they'll transition into the termination phase, and in the termination phase, there's a pretty big list of reasons why a Pod would shut down or terminate. First up, is the process finished, or crashed, right? We just either finished our execution and the process terminated, or something bad happened inside of our container‑based application, and it crashed and brought the Pod down. Another reason could be that the Pod is deleted, whether it be by a controller or administratively by a user at the command line. The next reason is a Pod could be evicted due to lack of resources on a node. In certain scenarios, if a node senses that it's running out of resources, it can elect for a particular Pod that's running on that node to be evicted, and it will be terminated and then recreated and rescheduled somewhere else in the cluster. We can also see terminations due to node failures or maintenance. If a node goes away, the workload that was on that node, well, that won't be there either. And so that will have to be recreated somewhere else and rescheduled in the cluster, or for maintenance reasons, perhaps as an administrator, we select to take a node offline for maintenance; that workload will have to move to somewhere else in the cluster based on the restart patterns of the controllers for those particular Pods. Now, I do want to call out that no Pod is ever re‑deployed. Pods are recreated onto the other resources in the cluster if they need to be rescheduled and recreated after a termination. We're going to look at the concepts behind Pod persistency a little bit later in this module. As we've learned so far in this course, Pods manage containers, which are just processes running inside of our cluster. And so when a Pod needs to be shut down, there is a process that happens, and the following is what occurs. Let's say we have a Pod that's up and running in our cluster, and a user or a controller sends a command to delete that Pod. The first thing that's going to happen is the API server is updated with a grace period timer, which by default is 30 seconds. The grace period timer is the time Kubernetes is allowing for the application to shut down gracefully on its own before it has to intervene. At this point in time, Pod status will show up as terminating when listed in client commands like kubectl. Next, the kubelet on the node that the Pod is running on sees that the Pod has been marked as terminating. It then sends a SIGTERM to the processes in the containers and those processes begin to terminate. The Pod is then removed from the endpoint list for any services that it may be associated with and is also removed from any controllers that it's associated with, and then the Pod is deleted. If for whatever reason, the processes in the containers in the Pods are still up and running when the grace period expires, those processes are going to be killed with a SIGKILL signal. Once those processes are killed, then that specific Pod resource is then removed from the API server and etcd is updated. The grace period timer is a configurable attribute. If you need to specify a grace period time for your application, you can do it in one of two ways, both imperatively at the command line and declaratively in code, and let's talk about both of those. The syntax to set a grace period timer for a Pod at the command line imperatively is kubectl delete Pod, specifying the Pod name, and then the parameter grace period, and then giving that a number of seconds. There are some scenarios where you might need to force the deletion of a Pod. A forced deletion immediately deletes a Pod from the API server and etcd. You'll need to do this in scenarios where you can't get the application processes to terminate, but you need to reuse that Pod name to get your application back online. A forced deletion looks like this: kubectl delete pod <name> ‑‑grace‑period=0 ‑‑force. Now it's important to note that you will have to go and clean up those non‑terminating processes still, but you get to reuse that Pod name to get your application back online. On the declarative side of things, recently Pod termination grace period has been added to the Pod spec and can be configured per Pod if needed by your application, and so it's possible that your app does take a bit of time to shut down. And in those scenarios, you'll want to increase Pod termination grace period from the default value of 30 seconds. And on the other hand, if you need to shorten Pod termination grace period, you can lower the value from the default value. The field in the Pod spec that you want to look for is terminationGracePeriodSeconds. Now, let's look at the persistency of Pods. We've said several times so far in this course and in this series of courses that Pods are never re‑deployed, they're recreated. If a Pod has stopped, either administratively or by a controller, when a new Pod is created, it's a brand‑new Pod. There's no state transition between the previous execution and the current one that is currently being recreated. This means that at Pod creation, the Pod goes back to the original container image that's part of the pod's definition. So that leaves us with the question, how do we deal with things like application configuration or application \_\_\_\_\_, stateful applications? If we're always going back to that initial container image state, how can we assert some level of persistency, configuration, or state between Pod lifecycles? Well, we have to decouple both state and configuration from the Pod, and Kubernetes gives us some constructs to do just that. So some of the core ways that Kubernetes gives us the ability to have persistency between Pod executions or Pod lifecycle is that configuration is managed externally to the Pod. So when we define our Pod manifests, we'll use things like secrets, which are stored in the cluster, or config maps, which are stored in the cluster as well. Both of these give us the ability to assert some level of application configuration onto our container‑based applications. We can use secrets to feed in things like passwords and other key values into our applications for things like connection strings, or we can use config maps to assert more complicated configuration constructs onto our container‑based applications. A very common technique for application configuration is passing environment variables into the containers at the Pod level. What this gives us the ability to do is at runtime, our applications will look for these environment variables, and then configure themselves appropriately based on the contents. Now, data persistency is managed externally to the Pod as well. We touched on this very slightly when we looked at the demonstration for multi‑container Pods. We learned that we can have a volume externally to our container‑based applications and then mount it inside of our containers. Well, that same construct can be used for data persistency. In that case, we used emptyDir. Well, there's other types of volumes that we can use that provide persistent volume access, things like PersistentVolumes and PersistentVolumeClaims which reference other types of storage, whether it be cloud‑based storage or physical storage in your data center. Now, both of these topics, both configuration management and data persistency are very, very deep topics. In fact, we'll have modules on each of these in upcoming courses, but I just wanted to cover that here within the construct of Pod lifecycle.

### Introducing and Working with Container Restart Policy

Inside of our pods, we have an additional layer of application resiliency, and that's the container restart policy. A container in a pod can restart independent of the pod. In fact, if our application crashes, or we initiate a restart on our own inside of our application, the pod itself really is just going to stay there for us and our container can restart. This is defined as part of the pod's spec. We tell Kubernetes in the pod's spec how we want our containers to react in the event of a container shutdown. Now, recall, the pod is the environment that our container executes or runs in. So, things like data persistency and configuration will still be available to our container‑based applications when they restart in the pod. Now, I do want to point out that our pods aren't going to be rescheduled on another node during container restarts, right? The pod is going to stay up and online during this period of time. The container restart is going to be initiated by the Kubelet on the node that that pod is running on. So our workload is going to stay where it is; it's just going to restart that container inside of the pod. Now, if our application is failing, and it's failing over and over and over again, Kubernetes protects itself and our applications with a restart exponential backoff, starting at 10 seconds, 20 seconds, 40 seconds, and capping itself at 5 minutes, and so it will slowly back off the time between restarts. After 10 minutes of successful runtime, this backoff loop is reset to 0. Container restart policy has three different configurations, and let's walk through each of those together right now. First up is Always, which is the default. The container restart policy always will restart all containers inside of a pod if they stop running. OnFailure will restart the containers on non‑graceful termination, basically non‑zero exit codes. And then Never, which means container restarts will never occur inside of a pod. Now let's look at what it takes to create a pod definition with the container restart policy. And just like any other pod definition, we start off with the API version in kind and some metadata. In this case, we're going to call this nginx‑pod. Then, in the pod spec, we'll still define our containers, so in this case, nginx, for the name and also the image. Now, here's where things get a little bit different. We define a restart policy at the pod level, not the container level. In this case, we're specifying the restartPolicy of OnFailure. So if something goes wrong inside of this container with the non‑zero exit code, it will restart the containers in this pod. In this case, there's only one container. If we don't specify a restart policy as part of our pod definition, the container restartPolicy is Always, which will restart the containers, regardless of how they stop or shut down.

### Demo: Pod Lifecycle and Container Restart Policy

Alright, so here we are in a demo, let's look at pod lifecycle, killing a container process, and seeing how it reacts with the container restart policy. Now in VS Code here, we have a session open to c1‑master1, and let's walk through some pod lifecycle events, killing a container process, and the container restart policy. And so like we have before, I'm going to kick off a kubectl get events, and I'm going to throw in the ‑‑watch parameter, and with an ampersand to background that task, because I want to show you what goes on when we work with some pods in Kubernetes and that pod lifecycle. And so let's go ahead and run that code, and we have some events from our last execution, so let's go ahead and run a clear command to clean that up to give us a nice, fresh console. The first thing that we're going to do together is create a pod, and I'm going to do that with kubectl apply ‑f pod.yaml. Inside of there is just our basic pod definition, and we see, we go through that workflow that we described in the pod lifecycle where we schedule the pod, and we go ahead and pull up the container, create the container, and start up the container. Let's go ahead and bring this next code into view. Now I'm going to go ahead and launch a shell into that container, because I want to see what's going on inside of that particular container‑based application, so kubectl exec ‑it, we're going to attach it to hello‑world‑pod, and the process we're going to execute is /bin/sh to give us a shell. Now inside of here, we have our process listing. So we see our Hello World app is up and running at PID 1, our shell on PID 9, and this ps process on PID 16. Now if we get out of there, and now we're going to make some fun stuff happen. Let's go ahead and execute a different command. We've used kubectl exec a lot to attach a shell. Well, we can use kubectl exec to launch an arbitrary process inside of a container if the binary is available to us inside that container. And so let's walk through what I have here on line 18. I say kubectl exec ‑it, there's our pod name, hello‑world‑pod, and the process that I'm going to run is /usr/bin/killall, and the parameter for that is hello‑app. So I'm going to go ahead and tell the container to kill that process inside that container. So let's go and do that together. Run this code. Now we see right away our kubectl get events watch shows us that, well, it created a new container and started the containers, right? So that's that container restart policy coming into play there. In that YAML file for pod.yaml, I didn't define a container restart policy. It defaults to Always, and it protects our application and keeps it up and running. And so let's look more closely at where we can see these other restart events in some other areas in our Kubernetes cluster. And so let's go ahead and grab kubectl get pods, and while you may have seen this Restarts column before in the output of kubectl get pods, well, guess what? That's exactly what that is. Usually we see this as 0, because no restarts have occurred, but in this case, we killed our application with killall, the container restart policy kicked in, restarted that container, and there we see one restart for that individual pod. If we wanted to look at this even more closely, we can use kubectl describe, and so let's go ahead and grab that kubectl describe output for our hello‑world‑pod. Now at the bottom in the events here, for the last three events, we see x2 or two times, over the last 2 minutes and 44 seconds. That means these three events at the bottom here executed more than once, in this case two times, over the last 2 minutes and 44 seconds, and the last events were 69 seconds ago, and those events were checking to see if the container image is there, creating the container, and starting the container, once for the initial pod creation and container starting, and then once again when we killed that application and the container restart policy kicked in. If we go up a little bit further on our output, we'll see some additional information underneath containers. So if you scroll up, in here you see containers, and then let's go ahead and scroll down a little bit further. We see that the state is currently running, and that it started a few minutes ago for this particular execution, so this one is at 9:55. The original container execution is in the last state. We see that original container execution is terminated, and the reason was error with an exit code of 2. That container started at 9:53 and then was killed by me at 9:55. So that 9:55 and 17 seconds is when I executed that killall command. Now up at the top there at 9:55 and 18 seconds is when this iteration of the container restarted. We see that our container Ready is set to True, so this container is up and running and ready, and the Restart Count was incremented to 1. And so we have some pretty good, detailed information here into the lifecycle of our containers. So let's go ahead and get a console on the bottom again here, and delete that pod. We're going to move on and work with some additional workloads here. So delete our pod, and once that's finished, we'll go ahead and kill our watch, we'll bring it into the foreground and throw a Ctrl+C at the console to get a terminal back. Now if you are working with pod definitions, you might forget where things live and things like that. Just a quick reminder here that you can use kubectl explain to get this kind of information, and in this case, kubectl explain pods.spec.restartPolicy. I run that there, and I can see the various settings for the restart policy and the output at the bottom there: Always, OnFailure, or Never in that the fact that it defaults to Always. So a good reminder of where we can go ahead and retrieve that information from. Let's go ahead and create some pods with specific restart policies, and I have some YAML over here to describe that. And so we have two pods being created; one is going to be hello‑world‑onfailure‑pod, and on line 9 there, we see the restartPolicy is specified as OnFailure. The second pod in this YAML file that we're going to create is hello‑world‑never‑pod, and on line 19 there, you can see the restart policy is going to be Never. And so we're going to go through, deploy these pods, and see how they react when we kill our application. So let's go ahead and jump back over to our demo, and send that code into our API server with kubectl apply ‑f pod‑restart‑policy.yaml, and at the bottom here, we can see both of our pods were created successfully. Just a quick check with kubectl get pods, and make sure everything is okay. We see hello‑world‑never‑pod and hello‑world‑onfailure‑pod; both are ready, so we see one of one, the status is running, and no restarts have occurred. So on line 45 here, let's go ahead and use that killall command again inside the container to execute the killall command against our hello‑app inside the pod, hello‑world‑never‑pod, and see how our restart policy reacts in this scenario. We'll go ahead and run this code, and do kubectl get pods. We can see that the hello‑world‑never‑pod status changes from Running to Error, right? Something went wrong. We can see in the Ready column that 0 of 1 containers is running, because we used killall to kill our application, and their container restart policy is set to Never. In this case, well, it certainly reacted the way that we wanted it to. And so let's go ahead and look at kubectl describe for this particular pod. And at the bottom there, we don't see any restart events this time, just the initialization events for the initial startup of this pod and its container. If we scroll up and look at our containers, again, so here we see Containers, and scroll down a little bit further, and we can look and see that our state is Terminated, our reason is Error, our exit code is 2, and basically our pod is not up and running. We also see that ready is set to False this time, and no restart count. So we didn't get our application back because we defined the restart policy as Never for this particular application. So let's go ahead and get our terminal back at the bottom and bring this code up. Now let's go ahead and do the same thing with our other pod, where we specify the container restart policies on failure. So we're going to send the killall command into our hello‑world‑onfailure‑pod to kill off the hello‑app again. Let's go ahead and run that. If we do kubectl get pods, this time, we'll see that our restart incremented to 1, and our status is running, so it recovered appropriately. We killed the process, the kubelet realized that that occurred, and then it kicked off a new creation of that container, set us back into the running status, and we see the increment that restarts to 1. Now let's go ahead and do that again. I'm going to kill our app one more time, and now I'm going to do a get pods again, and here we see we're in the error status for our onfailure pod. So let's think about what's occurring here. I killed the app, and then I killed it again, and what's happening is, well, that's the backoff loop asserting some control over the restart policy. So if we wait just a little bit, and we do kubectl get pods again, we should see running now. And so we just backed off for a second, restarted the container, and now we see restarts incremented to 2, and our status is set to 1. If you want to look at the detailed information behind what occurred there, we can use kubectl describe pod for our hello‑world‑onfailure‑pod, run that code there, and at the bottom, we can see that there have been three initializations of this container‑based application, the one in the initial pod deployment, the second one when we killed the process, and the third one when we killed the process again. So everything should be good to go here. Yes, that's a wrap for this demo. Let's go ahead and clean up our workload by deleting both of our pods there, and keep moving forward.

### Defining Pod Health: livenessProbes, readinessProbes and startupProbes

We just looked at container restart policy and how it's used to bring containers back online when they crash or something goes wrong. We can go a little bit further and help Kubernetes understand the health of our applications better. Let's discuss defining Pod health. In Kubernetes, a Pod is considered ready when all of the containers in that Pod are up and running. But maybe we need to give Kubernetes a little bit better of an understanding about what really defines a healthy and ready application in our container‑based apps that are running in Pods. We can add some additional intelligence to our Pod's state and health with container probes. With container probes, Kubernetes can know when the application in the Pod is up and responding properly before changing the Pod's state to ready. There are three types of container probes that we're going to look at today, livenessProbes, ReadinessProbes, and startupProbes. Let's get started. The first container probe that we're going to look at today is the livenessProbe. A livenessProbe continuously runs a diagnostic check on a container in the Pod, and this is a per container setting. And so if you are running multiple container Pods, you'll define a livenessProbe or a check for each container in your Pod, because it's possible that your containers in a Pod could have different definitions of health because they're different applications. Now, here's the key thing about livenessProbes. On failure, the kubelet will restart the container according to the container restart policy for that Pod. Using a livenessProbe, gives Kubernetes a better understanding of our application and how to react when things go wrong. And so, for example, if your application crashes and is no longer responding properly, Kubernetes will restart the container with the hopes of returning your application to service. You should, of course, figure out why your application crashed, but a restart is likely to get your container‑based application back into service quickly. The second type of container probe that we're going to look at today is the readinessProbe. ReadinessProbes continuously run diagnostic checks against the containers in your Pod during the life of the Pod to determine if your Pods are ready to receive traffic from Kubernetes services. ReadinessProbes can also help deployment rollouts ensure that there are a set number of Pods online and functioning to support your applications while rolling out new versions of Pods, and we'll dive into this in great detail in the course, Managing Controllers and Deployments. ReadinessProbes are per container settings, and so we'll define readinessProbes, if needed, for each container within a Pod. With the readinessProbe defined, on Pod start up, your application won't receive traffic from a service load balancer until the readinessProbe reports success, and on failure, the endpoint controller will remove the failed Pod's IP from the service endpoints for the service, so that it no longer receives traffic from the service load balancer. When using readinessProbes, on failure, the container isn't restarted. The Pod IP is removed from load balancing, and so that's how it differs from the livenessProbe, where a livenessProbe potentially restarts the container on failure, based on the container restart policy. ReadinessProbes are used to protect applications that temporarily can't respond to a request, and on failure, it removes the Pod endpoint from a service so that users won't see any application errors, since the traffic will be load balancing to the remaining pods that are up, running, and healthy with passing readinessProbes. The third type of container probe that we're going to look at today is the startupProbe. StartupProbes run diagnostic checks against the containers in your Pod during Pod startup to determine if the application containers in the Pod are up and ready for traffic. StartupProbes are a per container setting, and so we'll define them, if needed, for each container within a Pod. On Pod startup, when using a startupProbe, once the startupProbe succeeds, the Pod is considered ready, and so traffic will be load balanced to it or a deployment rollout can continue. If there are also readinessProbes and livenessProbes defined, they are disabled until the startupProbe succeeds. When that startupProbe succeeds, the liveness and readinessProbes are then enabled to help determine the Pod's liveness and readiness as they normally would. If no livenessProbes or readinessProbes are defined, then the Pod is considered ready at the point at which the startupProbe succeeds. On startupProbe failure, the kubelet will restart the container according to the container restart policy for the Pod. Now, you want to use startupProbes when you have applications that have long startup times. Before startupProbes existed in Kubernetes, people used a combination of liveness and readinessProbes to wait for slow application startup, and this led to more complex container probe configurations. Also, if the readinessProbes or livenessProbes fail too many times, it could cause the Pod to prematurely report as not ready and unnecessarily be restarted. The startupProbe helps solve these problems, since the startupProbe and livenessProbe and readinessProbes can each have different configurations defining what's healthy during Pod startup, and also what's healthy during when a Pod is up and running, and so you'll likely use them together to determine Pod startup success and runtime health. So far in this module, we've defined what container probes are, but we haven't defined what those diagnostic checks are that we could use in our container probes, and so let's go ahead and do that right now. Let's look at the types of diagnostic checks available for our container probes. There's three primary diagnostic checks available for container probes, and those are Exec, tcpSocket, and httpGet. Now, let's look at each one of these individually. The first one, Exec, measures a process exit code from an arbitrary process executed inside of our container‑based application. And so we simply execute a command in the container, and then look at the exit code and make a determination if our application is healthy or not. The second is tcpSocket. This is a test to see if we can successfully open a TCP socket on a container that we want to run the diagnostic check against. And then finally, httpGet . HttpGet will execute a check against the URL on the container, and looks at the return code for that request. If it's greater than or equal to 200 and less than 400, it's a successful check. Now for each of these diagnostic checks, there are three potential outcomes for our container probes. First up is success, that the container passed that check. Second is failure, that the container failed the check. And then the final state it is unknown. This is a case where the diagnostic check itself failed in its execution, and no action was taken against the container.

### Configuring and Defining Container Probes

Kubernetes gives us the ability to configure our container probes so that we can fine tune the readiness, liveness, and startup checks to be as accurate as possible to help us get a better representation of our application health. And so let's look at some of the ways that we can configure our container probes. The first configuration point is initialDelaySeconds. This is the number of seconds after the container has started up before it will start executing the container probes against the container. This can be useful if you need some additional time for your applications to start up before initializing the container probes. The default here is 0 seconds. Then there's periodSeconds. This is the probe interval or how frequently the container probe will run against a particular container. Next up is timeoutSeconds. Once a container probe has started executing against a container, this is how long the probe will wait before giving up and declaring failure. Then there's failureThreshold. This is the number of container checks that need to fail before the probe reports failure. Default here is 3. And then finally, successThreshold. After the failure of a container probe check, this is the number of probes that have to be successful before the application is considered healthy again. The default here is 1. Now let's look at the actual code to define a liveness and readinessProbe. We'll go through one example of each. Now let's say we're working at defining a Pod, and we're in the Pod spec. And we're in the containers section, and we're going about our business defining our container‑based application in terms of the name, an image, and whatever it takes else to construct that. In the containers section is where we'll define our liveness or readinessProbe. And let's start off with our first example here, which is going to be a livenessProbe. And I'm going to select for this particular livenessProbe a tcpSocket diagnostic check. For a tcpSocket diagnostic check, we'd have to declare a port. In this case, it's going to be 8080, just for our example. If I needed to add some additional configuration about this individual livenessProbe, here's where we'd go ahead and do that. So for this particular one, I'm going to have initialDelaySeconds set to 15, and the check interval or the periodSeconds set to 20 seconds. And so that's how we go about defining a livenessProbe in the container inside of a Pod spec. Now similarly, a readinessProbe would be established in the same area of our Pod's YAML inside of the container definition. And so for a readinessProbe, again, maybe you wanted to define a tcpSocket. We're going to define it on port 8080. It'll have an initial delay of 5 seconds and a periodSeconds of 10. And this is what it would look like to define both a liveness and readinessProbe in YAML as part of our Pod spec in the container definition. Now I do want to call out since we are looking at the code right here together that we can leverage both livenessProbes and readinessProbes together inside of a Pod spec for checking the health of a container. They're not mutually exclusive, and both could be leveraged to really extend and check the health of our container‑based applications. Now, let's check out the implementation details of a startup probe in YAML. And so inside of the Pod spec, inside of containers, we'll define a startupProbe. And similar to our livenessProbe that we just looked at, the startupProbe here is going to use a tcpSocket check, and we're going to tell it to check port 8080 and give it an initial delay of 10 seconds and a period of 5 seconds. And this startupProbe here, what it's going to do is when the Pod starts up, it'll start checking the container after a 10‑second initial delay, executing its checks every 5 seconds against TCP port 8080. Now I want to call out that we can leverage both livenessProbes and readinessProbes together with startupProbes inside of the Pod spec for checking the health of a container. They're not mutually exclusive, and each type of container probe can be leveraged to really extend and check the health of our container‑based applications. If you are using liveness and readinessProbes in your Pod spec in conjunction with a startupProbe, they're going to be disabled until that startupProbe is successful.

### Demo: Implementing Container Probes - livenessProbes and readinessProbes

Well, here we are in our demo, let's go ahead and look at how we can implement container probes. We're going to look at examples for livenessProbes, readinessProbes, and startupProbes. Here we are in VS Code, I have a session open to c1‑master1, and let's get started working with container probes. The very first thing that we're going to do here is kick off a kubectl get events with the ‑‑watch parameter, so we can watch how our containers react when livenessProbes or readinessProbes succeed and fail, and so let's go ahead and run this code here, get this cleared off to get us a fresh console, and get started. And so I've defined in this container‑probes.yaml file a deployment that has a pod template for a hello‑world container. That hello‑world container has two container probes defined, both a livenessProbe and a readinessProbe, and so both of those will work in conjunction to check the application health of this particular container‑based application. Our Hello World application lives on the container port 8080 as specified on line 19 there, and then on lines 20 and 25, we can see where we start the definitions of our liveness and readinessProbes. So, the livenessProbe that starts on line 20 is a tcpSocket probe, it's going to check for the health of port 8081. Now I'm intentionally using 8081 here and not matching to container port because I want to show you what a failed check looks like and then we're going to loop back and correct it, and show you what happens when the state changes and our application comes back online. On line 23, we configure an initialDelaySeconds of 10, and a periodSeconds of 5, that's our check control. Line 25 is where we begin our readinessProbe. In this case, I'm going to use an httpGet probe and check the URL at the path, just the base URL or the forward slash there on line 27, and again we're going to define a port here of 8081, we're telling our readinessProbe that our web application lives on 8081 when it actually lives on 8080, but again, I want to show you what a failure looks like, we'll correct it, and then we will get our application back up and running. Here, we'll set two parameters for the readinessProbe, initialDelaySeconds of 10 and periodSeconds of 5, and so let's jump back over to our demo and feed that into our API server with kubectl apply ‑f container‑probes.yaml. Run that code there and let's see what happens. So, just like we've seen before in other demos, since this is a deployment, it's going to scale that deployment's replica set from 0‑1, and bring our application up and online. After about 10 seconds, because of our initialDelaySeconds settings of 10 that we set in our container probe definition, we're going to start seeing, well, here we go, the readinessProbe and livenessProbes fail, and so let's go ahead and stop that before it runs off the screen too quickly. So bring that into the foreground, and go ahead and break out of the check, and so let's walk through this output together. So, in the output below from our kubectl get events, we can see the failed probe attempts of our liveness and readinessProbes and so let's walk through this output together and see what Kubernetes does when these checks start failing. And so, first off, we see three livenessProbes, Liveness probe failed: dial tcp, then the pod's IP on 8081: connect: connection refused, right? Our livenessProbe can't hit that tcpSocket on 8081, because, well, the application's actually running on 8080. We see a second livenessProbe check, a third livenessProbe check, and then Kubernetes reacts and kills the container, because of the failed livenessProbe. And so here we see, container will be killed and recreated, and then it goes through the process of killing the container, pulling the image, creating the container, and starting it up. In this output here, we also see two readinessProbe failures, and so that's trying to touch that web application and doing the HTTP Get on http://192.168.2.143:8081, again on the base URL, which we defined in our readiness check, and that's failing because we're pointing it to the wrong port. Now, let's go ahead and look at a couple other areas where we can see this information, and then we'll loop back and make the appropriate correction for this. So, when we do a kubectl get pods, we're used to seeing output that looks like this, but we're also used to seeing ready where it's 1/1 or the number of ready containers over the number of containers in the pod, in this case, there's one container, no containers are ready, so we see 0/1. We also see that we're in a CrashLoopBackOff because the livenessProbe is constantly killing the container and restarting it, trying to bring it back online. So here we can see five restarts in the last few minutes. So, we already kind of know what's wrong, but let's go ahead and keep moving forward and look at how we can look more closely at the runtime configuration and some of the events associated with this particular pod, and so I'm going to use kubectl describe pods to dig in a little bit further into what's going on with this particular container‑based application inside of our pod. So in the events, we can see lots of the events that we've seen so far at the bottom here. We can see Readiness probe failed, we can see Liveness probe failed, and the fact that it's looping over, creating our container and killing it over and over again. But if we go up in this output, we can go ahead and get some insight as to what's going on inside of here. If we go to our container definition, so we'll go all the way up to here, there's our container definition for our hello‑world container, then we'll loop back down, we can see the Liveness and Readiness probes configuration. So we see Liveness and we see tcp‑socket :8081, then we see the configuration, the delay=10s timeout=1s, and so on. We see the Readiness probe as an http‑get probe on http:// :8081/. Then we see its configuration in terms of delay and timeout, and so on for the attributes that we either configured in our YAML or are the default for that particular container probe. We see Ready: False, Restart Count: 5, so we know this thing is in bad shape, looking at what's going on in this output. We see the State as Waiting, the reason is because we're in the CrashLoopBackOff, and the Last State was terminated. Again, that livenessProbe just constantly restarting that container. So let's go ahead and fix that up. We'll jump over to our container‑probes.yaml definition, we'll switch it over to the correct ports, on line 22 we'll change the tcpSocket from 8081 to 8080, and we'll change the readinessProbe for the HTTP Get, we'll change its configuration to stop looking at 8081 and to start looking at 8080. Let's go ahead and save that out, jump back over to our script here, send that into our API server, get a new console at the bottom first, and then we'll send it into our API server, it configures our application. Now we're going to have to wait a little bit because of that CrashLoopBackOff for our application to come back online. Alright, so, before we go ahead and check to see if our application's online, let's go ahead and confirm that our configuration is correct for the container probe. So we do a kubectl describe pods, we'll scroll up to our readiness and livenessProbe configuration, there we see tcp‑socket :8080 and an http‑get :8080, so we're pointing at the right container port for our application, we can see the port in the container is 8080. So let's go ahead and bring up a new console and check the status of our application, do a kubectl get pods, we can see that we're now up and running with 1/1. I do want to point out that in this demo, we updated the deployment, which created a new replica set and that created a new pod, and that's what we see here, a new pod as part of that new replica set, with the readinessProbes and livenessProbes succeeding. So we're all done here, let's go ahead and clean up this demo with kubectl delete for our deployment and get moving forward.

### Demo: Implementing Container Probes - startupProbes

For our final demonstration, let's look at startup probes. The first thing that we're going to do is start up a watch on kubectl get events, and so here on line 50 we have the code to do just that. Kubectl get events ‑‑watch and we're going to background that task with an ampersand. Let's go ahead and clear our console to get rid of those old entries. Now what we're going to do next is we're going to deploy a deployment with a startup probe, and so let's go ahead and check out the code to do just that. So in the course downloads, you'll find container‑probes‑startup.yaml. And in here what we have is the deployment that will have defined a startup probe in. So inside of the Pod template spec and inside of the hello‑world container, you'll find here on line 20 a startupProbe. Now, this startupProbe is of type tcpSocket, and we see that there on line 21. The port that this startupProbe is going to check is port 8081. I'm intentionally injecting an error here so that we can watch the startupProbe in action and see what happens when it fails. The containerPort for our application is on port 8080. Now, in addition to the base configuration of the tcpSocket and the port, I'm also configuring a startupProbe with an initialDelaySeconds of 10, a periodSeconds of 5, and a failureThreshold of 1. In addition to the startupProbe, this deployment also has a livenessProbe and a readinessProbe configured. And so the livenessProbe is going to check tcpSocket port 8080, and the readinessProbe is going to do an httpGet against the base URL with the path of just the forward slash, and then the port is 8080. Now jumping back over to our driver script, let's go ahead and roll this code out with kubectl apply ‑f, and then specifying container‑probes‑startup.yaml. So there we run that code, and then the output at the bottom, what we can see is that deployment rollout, creating a Pod, then also creating the container. And so there we see Created container hello‑world, Started container hello‑world, and now in the output we see Startup probe failed because our startupProbe is pointing at the wrong port. In the output, we can see 8081 and that the connection was refused. Our application is actually running on port 8080. And so in a few moments here, what's going to happen since the startupProbe is failing, the container restart policy is going to kick in, and the container will be restarted by the kubelet. And after a few moments in the output, we can see that the container hello‑world failed startup probe and will be restarted. And then we see the process of creating and starting the container again. Now after that container restarts, the startup probe is still going to fail because we have an error in our configuration. So let's break out of this output at the bottom here and bring our kubectl events ‑‑watch into the foreground and kill that off. Before we fix that problem, let's do a kubectl get pods and inspect the output there. So there we see our Pod, and that status is running and it restarts as 1, and we saw that 1 restart in the output of kubectl get events. Additionally, I want to point out that we see ready 0/1. And so what that means is 0 of 1 containers defined inside of this Pod are up and ready because our startup probe is failing. And so let's go ahead and correct that issue. We'll jump back over to container‑probes‑startup.yaml and make the modification on the startupProbe, then change the tcpSocket check from port 8081 to 8080. So we'll save that out, jump back over to our driver script, and then roll that code out with kubectl apply ‑f container‑probes‑startup.yaml. And so if I do a kubectl get pods now, what we'll see is the old deployment is going away, and that should be terminated here in a second, I'll check the status in a moment. But during that time, that failed container restarted once more, so we see that incrementing from 1 to 2. Now, I also want to point out that the other Pod here, ending in gqg4k, that's part of our new fixed configuration, isn't up and running yet either because of the initial delay seconds of 10. So there we see it's not ready yet. If I do kubectl get pods once more, that status has changed. We can see that the previous Pod ending in 4469r is now terminating, so that'll shut down in a moment. And then our new Pod that's in our corrected deployment is now ready and we see that 1/1 containers in that Pod is reporting as ready. And so that is passing its startup probes and also passing those liveness and readiness probes that are configured in the Pod templet spec. And so that's a wrap for this demo, let's go ahead and delete our deployment.

### Module Summary and Thank You

Well, here we are at the end of our module, and we covered a lot of content. We looked at understanding Pods, what a Pod really is and what it looks like inside of our cluster. We then looked at the relationship between controllers and Pods. We then took some time to look at multi‑container Pods and how they interoperate and exchange data. We also covered managing Pod health with container probes so that we can help Kubernetes get a better understanding of our container‑based applications' health. And so here we are at the end of our course, and I really hope you enjoyed listening to this as we continue along in your Kubernetes studies. We've covered a lot of ground so far together. We did a deep dive into the Kubernetes API and the API server. We looked at some of the ways that you can manage objects with labels, annotations, and namespaces and how Kubernetes uses some of those constructs to manage its internals as well. Then we spent some time looking closely at Pods, what they're made of, and how we can help Kubernetes get a better understanding of our workloads. It's truly been a pleasure recording this course for you, and I thank you so much for listening and most importantly, learning with me. I hope you enjoyed the course, and join me again soon here, at Pluralsight.